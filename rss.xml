<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"><channel><title>Boston Women in Bioinformatics&apos;s Blog</title><description>The online presence of Women in Bioinformatics in the Boston Area</description><link>https://boston-wib.org</link><item><title>The Success Criteria Question: Why I Don&apos;t Start with Requirements</title><link>https://boston-wib.org/blog/quicktake/success-criteria-requirements</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/success-criteria-requirements</guid><description>&lt;p&gt;&lt;em&gt;The difference between requirements and success criteria&lt;/em&gt;&lt;/p&gt;

Most failed data projects have perfect requirements but wrong success criteria.
Here&apos;s what I mean:

Scientists rarely come to me with detailed requirements. They come with vague wishes: &quot;I wish we had historical data for our controls.&quot;

My job isn&apos;t to take that at face value. It&apos;s to dig deeper.

&quot;Really? You want this? Tell me more! What would you do with it?&quot;

&quot;Well, it would help us assess how our pipeline performs over time.&quot;

Now we&apos;re getting somewhere. She doesn&apos;t just want historical data - she wants to track quality trends and catch drift before it becomes a problem.

Lucky for her, I had designed our data warehouse to keep historical data neatly organized and right at my fingertips. A handful of days later, she had a web-based dashboard showing control performance over time. Her team was thrilled.

**The difference between requirements and success criteria:**

- Requirements describe WHAT someone wants
- Success criteria reveal WHY they need it and WHAT outcome they&apos;re trying to achieve

Scientists typically don&apos;t arrive with a spec sheet. They have a vague idea of what they want, shaped by whatever tools they&apos;re already comfortable with. My role is to bridge that gap - translate their scientific need into a technical solution they might not even know is possible.

This is why interdisciplinary communication matters so much. I need to understand their workflows well enough to recognize when I can show them something better or faster than their familiar approach.

**The key questions I ask:**

- What would you do with this?
- How would this change your workflow?
- What decision would this help you make?
- Walk me through what a typical day looks like now

These questions help me understand not just what they&apos;re asking for, but what they actually need. Sometimes the answer is exactly what they requested. Often, it&apos;s something adjacent that solves the underlying problem more elegantly.

And when you&apos;ve built your infrastructure with foresight - like keeping historical data organized from day one - you can deliver solutions in days instead of months.

What questions do you ask when someone brings you a vague request? How do you bridge between what people think they want and what they actually need?</description><pubDate>Mon, 27 Oct 2025 00:00:00 GMT</pubDate><category>bioinformatics</category><category>data-science</category><category>biotech</category><category>product-thinking</category><category>data-infrastructure</category></item><item><title>Why Every Series A Biotech Hits the Same Data Wall</title><link>https://boston-wib.org/blog/quicktake/why-every-series-a-biotech-hits-the-same-data-wall</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/why-every-series-a-biotech-hits-the-same-data-wall</guid><description>&lt;p&gt;&lt;em&gt;Avoid the common data pitfalls that slow down growing biotech companies.&lt;/em&gt;&lt;/p&gt;

You might be heading into a data crisis if:

- Your scientists spend 4+ hours per week on routine data tasks (compiling reports, finding files, waiting for someone else)
- The same question gets asked to your technical person 3+ times per week
- One person is the bottleneck for all data access or analysis
- Onboarding new scientists takes 2+ weeks before they can independently access their own data
- You&apos;re making hiring decisions based on data bottlenecks rather than scientific needs

If you checked 2 or more, you&apos;re already feeling it. At 3+, you&apos;re in the trap.

**How It Happens**

You close Series A. Team doubles overnight. Suddenly that one technical person who &quot;handled everything&quot; is drowning. Scientists who used to run their own analysis now have backlogs. All of the sudden, the funding celebration turns into a data crisis within 6 months.

The problem isn&apos;t lack of talent. It&apos;s three decisions that seem logical at the time:

&quot;We&apos;ll hire data people later&quot; You focus on science first. Makes sense, right? Except by Series B, scientists spend 8+ hours weekly on manual reporting. You&apos;re building infrastructure while running at full speed.

&quot;Let&apos;s buy an enterprise platform&quot; You invest 6 figures in software built for 200-person companies. Your 15 scientists can&apos;t actually use it. Everyone goes back to spreadsheets.

&quot;We&apos;ll standardize once we&apos;re bigger&quot; Six months later, three people run the same analysis three different ways. Nobody knows the source of truth. Onboarding takes weeks because everything is tribal knowledge.

**What Actually Works**

Companies that scale smoothly do three things:

- Identify their biggest bottleneck and solve it first with a targeted tool
- Create just enough process to prevent chaos without slowing science
- Build systems that make both scientists and technical teams look good

The goal isn&apos;t perfect data infrastructure. It&apos;s removing obstacles that keep brilliant scientists from doing brilliant science.

Start small. Pick one bottleneck. Build one self-service tool. Free your team from one repetitive task.

That&apos;s how you avoid the trapâ€”one strategic decision at a time.

Have you experienced the Series A data crunch? What was your biggest bottleneck when your team doubled?</description><pubDate>Thu, 23 Oct 2025 00:00:00 GMT</pubDate><category>biotech</category><category>data-science</category><category>series-a</category><category>data-infrastructure</category><category>startups</category></item><item><title>Your Proof-of-Concept Is Not A Platform</title><link>https://boston-wib.org/blog/quicktake/your-poc-is-not-a-platform</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/your-poc-is-not-a-platform</guid><description>&lt;p&gt;&lt;em&gt;The best Proof-of-Concepts teach you what to build next and then get retired.&lt;/em&gt;&lt;/p&gt;

Everyone loves building a proof of concept (POC). It&apos;s cheap, low stakes, and nobody expects it to do everything.

The problem? The POC never dies.

Here&apos;s what actually happens:

You build a POC. Parts of it work great. Parts of it... well, you ignore those parts and build the next thing you need on top of it.

Then someone needs something else. You bolt that on too.

Six months later, you&apos;re calling it &quot;The Platform.&quot;

But really, it&apos;s a Frankenstein monster of every POC you ever built, duct-taped together. The database schema makes no sense because it&apos;s solving five unrelated problems. The code is a maze that only one person understands. You&apos;re one resignation away from disaster.

**How this happens**:

- POC works well enough, so why start over?
- Adding to existing code is faster than building from scratch
- You don&apos;t realize you&apos;re building a platform until you already have one
- Each addition makes sense in isolation
- Nobody wants to be the person who says &quot;we need to rebuild this&quot;

**What you should do instead**:

- Treat each POC as disposable. If you&apos;re keeping it, it&apos;s not a POC anymoreâ€”rebuild it properly.
- Modularize as you develop. Pull utility functions into their own reusable modules from the start.
- New functionality = new POC. Don&apos;t bolt it onto the old thing just because it&apos;s there.
- Accept the upfront cost. Yes, starting fresh takes longer. Yes, it&apos;s worth it.
- Name things honestly. If you&apos;re maintaining it long-term, stop calling it a POC. Acknowledge you&apos;re building infrastructure.

I&apos;ve seen this pattern a lot. The POC that becomes &quot;The Platform&quot; is usually the most fragile, hardest-to-maintain piece of infrastructure in the entire organization.

The best POCs teach you what to build nextâ€”then get retired.

What POC is haunting your codebase right now? ğŸ‘» Have you inherited a POC-turned-platform? What&apos;s your strategy for untangling it?</description><pubDate>Mon, 20 Oct 2025 00:00:00 GMT</pubDate><category>data-engineering</category><category>technical-debt</category><category>software-architecture</category><category>software-engineering</category><category>data-infrastructure</category></item><item><title>The Universal Pattern Behind Scalable Data Systems</title><link>https://boston-wib.org/blog/quicktake/he-universal-pattern-behind-scalable-data-systems</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/he-universal-pattern-behind-scalable-data-systems</guid><description>After building data systems in genomics, clinical trials, and synthetic biology, I&apos;ve noticed the same architecture pattern emerging again and againâ€”even in completely different industries.

The pattern:

- Multiple data sources (APIs, sensors, external databases)
- Automated integration and analysis
- User-facing reports that non-technical experts can act on immediately

**Why this pattern matters**:

I once built a quality control pipeline that integrated sequencing data, lab automation outputs, and historical performance metrics. As a result, scientists could troubleshoot failed experiments in minutes instead of days, and they didn&apos;t need to understand the underlying computational complexity.

I also once created a clinical trials dashboard that pulled from multiple APIs and transformed scattered data into visual insights. Research teams went from waiting days for analysis to exploring data independently in real-time.

Recently, I&apos;ve been exploring opportunities outside biotechâ€”and I keep seeing the exact same problem: domain experts drowning in data from multiple sources, needing insights fast, but lacking the computational tools to connect the dots.

**The transferable skills**:

The sensors change (sequencing instruments vs IoT devices vs API feeds), but the architecture stays the same:

- Design data storage that scales from prototype to production
- Build pipelines that handle messy real-world data gracefully
- Create reports that build trust through clarity and consistency
- Ship fast, iterate based on user feedback, then optimize

**What I&apos;ve learned**:

Domain expertise takes years to build. But if you can bridge the gap between complex data and the people who need insights from it, you become valuable in any industry that&apos;s drowning in data but starving for actionable information.
That&apos;s every industry right now.

If you&apos;re building data systems that integrate multiple sources and serve non-technical users, I&apos;d love to hear what patterns you&apos;re seeing. What stays the same across domains? What actually changes?</description><pubDate>Thu, 16 Oct 2025 00:00:00 GMT</pubDate><category>data-engineering</category><category>bioinformatics</category><category>career-transition</category></item><item><title>A Coffee with CompBio: Fail, learn, repeat, the bioinformatics way!</title><link>https://boston-wib.org/blog/coffeewithcompbio/episode-9</link><guid isPermaLink="true">https://boston-wib.org/blog/coffeewithcompbio/episode-9</guid><description>&lt;p&gt;&lt;em&gt;A coffee with Saranya Canchi&lt;/em&gt;&lt;/p&gt;

import { FaBell, FaSpotify, FaApple } from &apos;react-icons/fa&apos;;

&lt;iframe
  name=&quot;Ausha Podcast Player&quot;
  frameborder=&quot;0&quot;
  loading=&quot;lazy&quot;
  id=&quot;ausha-NiRp&quot;
  height=&quot;220&quot;
  style=&quot;border: none; width:100%; height:220px&quot;
  src=&quot;https://player.ausha.co/?podcastId=6w55XHmZ1JAq&amp;playlist=false&amp;color=%23751cbf&amp;v=3&amp;playerId=ausha-NiRp&quot;
&gt;&lt;/iframe&gt;
&lt;script src=&quot;https://player.ausha.co/ausha-player.js&quot;&gt;&lt;/script&gt;

Grab your coffee and join us for another episode of **A Coffee with Comp Bio**!

In this episode of A Coffee with Comp Bio, hosts Alex Bartlett and Lorena Pantano sit down with Saranya Canchi, a computational biologist specializing in neuroscience. Together, they explore how to thrive as a self-directed learner in bioinformaticsâ€”tackling early challenges, learning through projects, and building problem-solving resilience. Saranya shares her journey as a self-taught bioinformatician, highlighting the importance of mastering the fieldâ€™s unique language and embracing failure as part of growth. Whether youâ€™re just starting out or looking to strengthen your learning approach, this conversation offers practical insights and inspiration for your bioinformatics journey.

Saranya&apos;s webpage: [https://s-canchi.github.io/](https://s-canchi.github.io/)

Send us your comments, questions, and suggestions using [this form](https://forms.gle/ncwo6HZeN4uA9gPg7)

Thanks [Amulya Shastr](https://www.linkedin.com/in/amulya-shastry/) for editing and management support.

Listen to this podcast on other platforms:

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaApple size={32} className=&quot;text-black dark:text-white&quot; /&gt;{&apos; &apos;}
    &lt;a href=&quot;https://podcasts.apple.com/us/podcast/a-coffee-with-saranya-canchi-fail-learn/id1817024741?i=1000731855297&amp;uo=4&quot;&gt;
      Apple
    &lt;/a&gt;
  &lt;/span&gt;

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaSpotify size={32} color=&quot;#1DB954&quot; /&gt;{&apos; &apos;}
    &lt;a href=&quot;https://open.spotify.com/episode/2Sx6bFIOAt3x32o5IvySAu&quot;&gt;Spotify&lt;/a&gt;
  &lt;/span&gt;

We are looking for sponsors! Please get in touch if you or your business would like to help support this podcast.

Follow [Lorena](https://www.linkedin.com/in/lpantano) and [Alex](https://www.linkedin.com/in/alexandra-bartlett-926b32109) on LinkedIn!

If you enjoyed the episode, [please subscribe and leave a review!](https://podcast.ausha.co/a-coffee-with-compbio)

Hosted by Ausha. See [ausha.co/privacy-policy](https://ausha.co/privacy-policy) for more information</description><pubDate>Tue, 14 Oct 2025 00:00:00 GMT</pubDate><category>science-communication</category><category>neuroinformatics</category><category>self-directed-learning</category><category>bioinformatics</category><category>problem-solving-skills</category></item><item><title>The Sphere of Inluence in Project Management</title><link>https://boston-wib.org/blog/quicktake/the-sphere-of-influence-in-project-management</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/the-sphere-of-influence-in-project-management</guid><description>&lt;p&gt;&lt;em&gt;How to focus your energy to actually make a difference&lt;/em&gt;&lt;/p&gt;

Early in my career, I&apos;d get frustrated when projects stalled because of things &quot;outside my control.&quot;

A stakeholder wouldn&apos;t prioritize our requirements. Another team&apos;s timeline slipped. Budget decisions happened three levels above me.
I was treating project management like a binary: either I controlled something, or I was powerless.

That&apos;s not how influence actually works.

**The three zones of influence:**

- **Direct control**: Your team&apos;s execution, your own priorities, technical decisions within your scope. This is where you set standards and drive outcomes.
- **Influence**: Stakeholder priorities, cross-team dependencies, resource allocation. You can&apos;t force outcomes here, but you can shape them through relationships and strategic framing.
- **Concern**: Market conditions, executive strategy, company-wide decisions.

You need awareness here, but spinning your wheels trying to change these things drains energy from where you can actually make a difference.

The insight that changed my approach: most project success happens in the influence zone.

When I stopped trying to control stakeholder timelines and instead focused on understanding their constraints, I could frame our project as solving their problem. That&apos;s influence.

When dependencies with other teams became blockers, I stopped escalating and started asking &quot;what would make this easier for your team?&quot; Often, a small adjustment on our side unlocked everything. That&apos;s influence.

When budget discussions happened above my level, I made sure my manager had clear data on ROI and risk, framed in terms of their goals. That&apos;s influence.

**The practical shift:**

For your current project, list everything that feels like a blocker. Then honestly categorize each one:

- What do you actually control?
- What can you influence?
- What&apos;s in the concern zone?

Stop spending energy in the concern zone. Double down on the influence zone.
The most effective project leaders I&apos;ve worked with don&apos;t have more authority than anyone else. They&apos;re just exceptionally skilled at understanding what motivates each stakeholder and connecting those motivations to project success.

That&apos;s the power of operating in your sphere of influence.

Where do you find yourself spending most of your energy? In your control zone, influence zone, or spinning your wheels on things you can only be concerned about?</description><pubDate>Thu, 09 Oct 2025 00:00:00 GMT</pubDate><category>project-management</category><category>sphere-of-influence</category><category>leadership</category></item><item><title>Precision Medicine, Through One Lens</title><link>https://boston-wib.org/blog/quicktake/precusion-medicine-through-one-lens</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/precusion-medicine-through-one-lens</guid><description>&lt;p&gt;&lt;em&gt;Bridging Biology with Clinical Insight&lt;/em&gt;&lt;/p&gt;

Precision medicine is more than just a buzzword; itâ€™s a shift in how we understand, treat, and even predict disease. Instead of using a â€œone-size-fits-allâ€ approach, it uses data from a patientâ€™s genome to their clinical history to design treatments that are tailored to them.

âœ¨ While precision medicine is a broad field that also involves environmental, lifestyle, and behavioral factors, biological and clinical data play a key role in shaping how we design targeted treatments and predict patient outcomes.

#PrecisionMedicine is about asking deeper questions:

- Why do two patients with the same diagnosis respond differently to the same therapy?
- Which genetic signatures predict how someone will react to a drug?
- How can we use molecular data to catch diseases before they appear in symptoms?

And the answer lies in #DataScience. By integrating molecular datasets (like gene expression or mutations) with clinical data (like outcomes or lab results), precision medicine connects the â€œwhyâ€ from biology with the â€œwhatâ€ from patient care.

This isnâ€™t the future; itâ€™s already reshaping how we diagnose, treat, and prevent disease.

ğŸ’¡ Want to see how clinical insights and biological data come together in practice?

ğŸ‘‰ğŸ» I dive deeper into this idea of this integration with a real-world example [here](https://medium.com/@riyadua99/precision-medicine-where-biology-meets-clinical-insights-c14b707f1195).

ğŸ’­ Beyond biological and clinical data, which datasets do you believe could further advance the precision medicine revolution?</description><pubDate>Wed, 08 Oct 2025 00:00:00 GMT</pubDate><category>bioinformatics</category><category>clinical-informatics</category><category>data-science</category><category>genomics</category><category>healthcare-data</category><category>precision-medicine</category><category>computational-biology</category><category>patient-care</category></item><item><title>Multi-Source Data Integration</title><link>https://boston-wib.org/blog/quicktake/multi-source-data-integration</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/multi-source-data-integration</guid><description>&lt;p&gt;&lt;em&gt;From chaos to clarity: How the medallion architecture transforms messy, multi-source data into trustworthy insights.&lt;/em&gt;&lt;/p&gt;

_&quot;Just pull the data from Benchling.&quot;_

If only it were that simple.

You&apos;re also pulling from three different sequencing platforms, two legacy Excel trackers someone maintains &quot;just in case,&quot; and a PostgreSQL database with a schema that was designed before anyone on the current team started.
Each source has different naming conventions. Different update frequencies. Different levels of trust. And somehow, you need to combine all of this into something your scientists can actually use.
This is where I&apos;ve found the medallion architecture invaluable.

## **What is the mendallion architecture?**

It&apos;s a systematic way to transform messy, multi-source data into something trustworthy and useful. Think of it as three progressive layers of refinement:

### ğŸ¥‰ **Bronze Layer**: Raw data, exactly as it arrives

- One table per source, no transformations
- Your source of truth for &quot;what did we actually receive?&quot;
- Preserves everything, even the weird edge cases

### ğŸ¥ˆ **Silver Layer**: Cleaned and standardized data

- Consistent naming conventions across all sources
- Data quality checks and validation rules applied
- Schema harmonization (finally, all your sample IDs match!)
- This is where you fix the &quot;sample_id&quot; vs &quot;sampleID&quot; vs &quot;Sample_Identifier&quot; problem

### ğŸ¥‡ **Gold Layer**: Business-ready data

- Domain-specific logic applied
- Aggregations and calculations complete
- Ready for scientists to query directly
- This is what powers your dashboards and tools

## **Why this matters**

When you serve data from the gold layer, your users don&apos;t need to know that their simple query is actually reconciling data from five different sources. They don&apos;t need to remember which system uses underscores and which uses camel case. They just get the answer they need.
And when something breaks upstream? You can trace it back through the layers without touching production data.
The bronze layer lets you say &quot;here&apos;s exactly what we received&quot; when debugging. The silver layer ensures consistency. The gold layer delivers value.

## **The practical impact**

This architecture transformed how we handled data integration at Korro. Instead of constantly troubleshooting why two datasets didn&apos;t align, we had clear stages where we could pinpoint exactly where problems originated.
More importantly, it meant scientists could trust the data they were seeing. When everything is standardized in gold, they can focus on the science instead of data wrangling.

For those of you managing multiple data sources: How are you handling schema mismatches and data reconciliation? Are you building transformations on the fly, or do you have a structured approach?</description><pubDate>Tue, 07 Oct 2025 00:00:00 GMT</pubDate><category>data-engineering</category><category>tech-leadership</category><category>remote-first</category><category>engineering-management</category></item><item><title>Managing Data Engineering Consultants Across 4 Time Zones: What Actually Worked</title><link>https://boston-wib.org/blog/quicktake/managing-data-engineering-consultants-across-four-timezones</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/managing-data-engineering-consultants-across-four-timezones</guid><description>&lt;p&gt;&lt;em&gt;Strategies to management without real-time communication&lt;/em&gt;&lt;/p&gt;

When I took on managing a distributed team of 6 consultants across 4 time zones, I quickly realized the traditional management playbook wouldn&apos;t work. No amount of daily standups would solve the fundamental challenge: we couldn&apos;t rely on real-time communication.

Success came down to one thing: the team needed to be independent enough not to require constant hand-holding.

**Remote-First, Not Remote-Friendly**

I built the team around async-first communication. Yes, I was always available for meetings, but we defaulted to asynchronous methods:

- Jira for task tracking and context
- Confluence for decisions and architecture docs
- GitHub for code reviews and technical discussions

This wasn&apos;t about avoiding meetings - it was about respecting that someone in Bangkok shouldn&apos;t have to wait until Boston wakes up to unblock their work.

**The &quot;1 hour Rule&quot;** ğŸ•

Here is a good rule of thumb: if you&apos;re stuck on a problem for about an hour, reach out for help. There are no laurels for spending days wrestling with an issue alone. The goal wasn&apos;t to eliminate struggle - it was to prevent the kind of invisible blocking that kills distributed team productivity.

**The Smart &quot;Buy&quot; Decision**

One of the best investments we made was implementing Sentry for error tracking. When our data pipelines threw errors, they sent detailed information to Sentry&apos;s dashboard automatically.

This meant team members across time zones could:

- Check on issues asynchronously
- See error patterns without digging through logs
- Understand context before reaching out for help

It was a perfect example of &quot;build vs buy&quot; done right - we bought the infrastructure for distributed awareness so we could focus on building what mattered.

**What Made It Work**

The distributed setup succeeded because we optimized for independence:

- Comprehensive documentation that answered the &quot;why&quot; not just the &quot;what&quot;
- Clear ownership boundaries so people knew when to make decisions vs escalate
- Tooling that created shared visibility without requiring shared schedules
- A culture that valued asking for help as much as figuring things out

Managing distributed teams isn&apos;t about controlling what happens across time zones. It&apos;s about creating systems where talented people can do their best work independently, while still feeling connected to the team&apos;s goals.

What challenges have you faced managing distributed data engineering teams? What tools or practices made the biggest difference?</description><pubDate>Thu, 02 Oct 2025 00:00:00 GMT</pubDate><category>data-engineering</category><category>tech-leadership</category><category>remote-first</category><category>engineering-management</category></item><item><title>The API Strategy Gap in Research</title><link>https://boston-wib.org/blog/quicktake/the-api-strategy-gap-in-research</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/the-api-strategy-gap-in-research</guid><description>&lt;p&gt;&lt;em&gt;Most biotech companies build applications. Few build APIs.&lt;/em&gt;&lt;/p&gt;

I once worked on a QC app that let scientists explore fresh sequencing data with statistical tools. Great for analysis. Terrible for integration ğŸ«£

Then we added one API endpoint.

Now when scientists completed their QC review, those results automatically flowed back into our data warehouse. Other teams could access QC insights without asking for exports. The manual human judgment that only scientists could provide became part of our institutional knowledge.

One API call eliminated a major data silo ğŸš€

**ğ— ğ—¼ğ˜€ğ˜ ğ—¯ğ—¶ğ—¼ğ˜ğ—²ğ—°ğ—µ ğ—°ğ—¼ğ—ºğ—½ğ—®ğ—»ğ—¶ğ—²ğ˜€ ğ—¯ğ˜‚ğ—¶ğ—¹ğ—± ğ—®ğ—½ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€. ğ—™ğ—²ğ˜„ ğ—¯ğ˜‚ğ—¶ğ—¹ğ—± ğ—”ğ—£ğ—œğ˜€.**

Each beautiful application becomes a dead end instead of a building block. I&apos;ve watched organizations rebuild the same data transformations five times because no one designed for reusability.

- Design APIs before applications

- Make data transformations reusable components

- Build for the analyst you don&apos;t have yet

- Document how to extend, not just how to use

Are you building applications or building platforms?</description><pubDate>Mon, 29 Sep 2025 00:00:00 GMT</pubDate><category>softwareEngineering</category><category>data-engineering</category><category>API-design</category><category>biotech</category><category>data-architecture</category><category>data-integration</category></item><item><title>Change management in small biotech</title><link>https://boston-wib.org/blog/quicktake/change-management-in-small-biotech</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/change-management-in-small-biotech</guid><description>&lt;p&gt;&lt;em&gt;The &quot;slow down to speed up&quot; paradox&lt;/em&gt;&lt;/p&gt;

In my years across small biotechs, &quot;change management&quot; felt like a luxury we couldn&apos;t afford. The mantra was always speed: &quot;Quick patch before the board meeting,&quot; &quot;Just get the analysis out the door,&quot; &quot;We&apos;ll fix it properly later.&quot;

But here&apos;s what I learned: Emergency patches don&apos;t scale.

When your computational biologist leaves and takes all the tribal knowledge about why that one script has three different output formats... when your &quot;quick fix&quot; becomes the production system that processes clinical trial data... when you&apos;re debugging the same pipeline failure for the fourth time this month...

That&apos;s when you realize change management isn&apos;t bureaucracy. It&apos;s infrastructure.

**What would &quot;change management light&quot; actually look like?**

- Pre-mortems for major releases - 15 minutes asking &quot;What could go wrong?&quot; before pushing to production

- Change logs that explain why - Not just what changed, but what problem it solved

- Rollback plans for critical systems - Because 3am is not when you want to figure out deployment dependencies

- Cross-training documentation - So knowledge doesn&apos;t walk out the door with departing team members

Change management in biotech isn&apos;t about slowing down innovation. It&apos;s about building confidence to move faster.

When your data pipeline has proper testing and rollback procedures, you can actually deploy more frequently. When your analysis scripts have clear documentation, new team members can contribute in weeks instead of months. When you have change logs, debugging becomes archaeology instead of guesswork.

**The trade-off is real**: You spend 20% more time on process to save 80% of your time on firefighting.

For small biotechs, the question isn&apos;t whether you can afford to implement change management. It&apos;s whether firefighting is your long-term strategy - especially as you scale, face regulatory scrutiny, or need to onboard new team members quickly.

What&apos;s your experience with balancing speed vs. process in biotech? Have you seen change management approaches that actually accelerated rather than slowed down scientific progress? ğŸ¤”</description><pubDate>Thu, 25 Sep 2025 00:00:00 GMT</pubDate><category>biotech</category><category>change-management</category><category>data-engineering</category><category>bioinformatics</category><category>scientific-computing</category></item><item><title>A Coffee with CompBio: (Dry) Lab Notebooks</title><link>https://boston-wib.org/blog/coffeewithcompbio/episode-8</link><guid isPermaLink="true">https://boston-wib.org/blog/coffeewithcompbio/episode-8</guid><description>&lt;p&gt;&lt;em&gt;The Importance of Recordkeeping in CompBio with insights from Amulya Shastry and Lina Faller.&lt;/em&gt;&lt;/p&gt;

import { FaBell, FaSpotify, FaApple } from &apos;react-icons/fa&apos;;

&lt;iframe
  name=&quot;Ausha Podcast Player&quot;
  frameborder=&quot;0&quot;
  loading=&quot;lazy&quot;
  id=&quot;ausha-kKTp&quot;
  height=&quot;220&quot;
  style=&quot;border: none; width:100%; height:220px&quot;
  src=&quot;https://player.ausha.co/?podcastId=VLQQwSdNm6Dm&amp;playlist=false&amp;color=%23751cbf&amp;v=3&amp;playerId=ausha-kKTp&quot;
&gt;&lt;/iframe&gt;
&lt;script src=&quot;https://player.ausha.co/ausha-player.js&quot;&gt;&lt;/script&gt;

Grab your coffee and join us for another episode of **A Coffee with Comp Bio**!

This time, Alexandra Bartlett and I kick things off with Amulya Shastry, a PhD student at Boston University and co-chair of Boston Women in Bioinformatics, who introduces us to **llmr -- a new Tidyverse-friendly** tool for connecting with LLMs like ChatGPT, Gemini, and more.

Then we sit down with Lina L. Faller, Ph.D., a veteran in bioinformatics with nearly two decades of experience bridging software engineering, research, and pharma. Lina shares **why she started blogging about sustainable data systems, leadership in tech, and the very human side of computational biology**. We dive into one of her favorite topics: **why computational biologists should keep lab notebooks** (yes, even if your &quot;lab&quot; is just a laptop). From reproducibility to institutional memory to the art of &quot;**forensic bioinformatics**,&quot; Lina brings stories and advice that will be useful to anyone working with data.

If youâ€™ve ever forgotten what you coded six months ago (weâ€™ve all been there), or wondered how AI might fit into documentation and knowledge-sharing, this episode is for you.

Send us your comments, questions, and suggestions using [this form](https://forms.gle/ncwo6HZeN4uA9gPg7)

[https://ellmer.tidyverse.org/articles/ellmer.html](https://ellmer.tidyverse.org/articles/ellmer.html)

[https://lfaller.github.io/](https://lfaller.github.io/)

Thanks [Amulya Shastr](https://www.linkedin.com/in/amulya-shastry/) for editing and management support.

Listen to this podcast on other platforms:

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaApple size={32} className=&quot;text-black dark:text-white&quot; /&gt;{&apos; &apos;}
    &lt;a href=&quot;https://podcasts.apple.com/us/podcast/dry-lab-notebooks-the-importance-of-recordkeeping/id1817024741?i=1000728035337&quot;&gt;
      Apple
    &lt;/a&gt;
  &lt;/span&gt;

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaSpotify size={32} color=&quot;#1DB954&quot; /&gt;{&apos; &apos;}
    &lt;a href=&quot;https://open.spotify.com/episode/4QBmN9kfRkEqmKUimqY9nx?si=231bb176b6be456d&quot;&gt;Spotify&lt;/a&gt;
  &lt;/span&gt;

We are looking for sponsors! Please get in touch if you or your business would like to help support this podcast.

Follow [Lorena](https://www.linkedin.com/in/lpantano) and [Alex](https://www.linkedin.com/in/alexandra-bartlett-926b32109) on LinkedIn!

If you enjoyed the episode, [please subscribe and leave a review!](https://podcast.ausha.co/a-coffee-with-compbio)

Hosted by Ausha. See [ausha.co/privacy-policy](https://ausha.co/privacy-policy) for more information</description><pubDate>Tue, 23 Sep 2025 00:00:00 GMT</pubDate><category>science-communication</category><category>tidyverse</category><category>sustainable-data-systems</category><category>llms</category><category>ai-in-bioinformatics</category></item><item><title>Force Multiplication Through Simple Solutions</title><link>https://boston-wib.org/blog/quicktake/force-multiplication-through-simple-solutions</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/force-multiplication-through-simple-solutions</guid><description>&lt;p&gt;&lt;em&gt;The best tech solutions aren&apos;t rocket science. They&apos;re force multipliers.&lt;/em&gt;&lt;/p&gt;

One time, a scientist approached me: &quot;Can I see how our controls have performed over time?&quot;

Within a day, they had a simple scatter plot showing months of control performance data. They immediately dove in and explored trends and patterns that emerged.

But here&apos;s what they didn&apos;t see: the months of &quot;boring&quot; infrastructure work that made that &quot;simple&quot; request possible.

**What the scientist saw:**

- Quick answer to their question

- Clean, intuitive visualization

- Immediate insights

**What actually happened:**

- Strategic data warehouse design enabled rapid querying

- ETL pipelines ensured data quality and consistency

- Historical data architecture anticipated future questions

- APIs made complex time-series analysis feel effortless

The result? They could test new hypotheses immediately instead of waiting weeks for data extraction. Control issues that might have gone unnoticed for months were caught early.

This is why I&apos;ve learned to love infrastructure work. Not because databases are glamorous, but because the right foundation transforms impossible questions into simple SQL queries.

When scientists can get answers in minutes instead of days, they don&apos;t just save time. They explore more hypotheses, iterate faster, and make discoveries they couldn&apos;t before.

The most impactful technical work often feels invisible. You&apos;ve absorbed all the complexity so your users experience simplicity.

What &quot;simple&quot; solutions in your work have had outsized impact? I&apos;d love to hear about the infrastructure wins that enabled quick victories.</description><pubDate>Mon, 22 Sep 2025 00:00:00 GMT</pubDate><category>data-infrastructure</category><category>bioinformatics</category><category>tech-leadership</category><category>scientific-software</category></item><item><title>Every Quick Fix in Research Code is a Future Investment Decision</title><link>https://boston-wib.org/blog/quicktake/every-quick-fix-in-research-code-is-a-future-investment-decision</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/every-quick-fix-in-research-code-is-a-future-investment-decision</guid><description>&lt;p&gt;&lt;em&gt;The challenge isn&apos;t eliminating quick fixesâ€”it&apos;s being intentional about which ones you keep and how you document the journey.&lt;/em&gt;&lt;/p&gt;

In biotech, the pressure to deliver results &quot;by Friday&apos;s meeting&quot; often creates shortcuts that become permanent infrastructure. I&apos;ve seen analysis scripts become production pipelines, temporary databases become data warehouses, and proof-of-concept tools become mission-critical systems.

But here&apos;s the hidden cost: each quick fix makes the system harder to teach. The more convoluted a project becomes, the more it creates knowledge silos. Suddenly, one person becomes the &quot;go-to&quot; for maintaining that tool, and it becomes incomprehensible to everyone else.

This isn&apos;t just technical debtâ€”it&apos;s ğ—¼ğ—¿ğ—´ğ—®ğ—»ğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ—®ğ—¹ ğ—±ğ—²ğ—¯ğ˜. You&apos;ve accidentally created a single point of failure disguised as expertise.

The challenge isn&apos;t eliminating quick fixesâ€”it&apos;s being intentional about which ones you keep and how you document the journey.

- Document the shortcuts you take (and why)

- Schedule regular &quot;technical debt audits&quot;

- Test knowledge transfer before it becomes critical

- Budget time for converting prototypes to production

- Make the invisible costs visible to stakeholders

How do you balance &quot;ship fast&quot; with &quot;ship sustainably&quot; while keeping your tools teachable?</description><pubDate>Thu, 18 Sep 2025 00:00:00 GMT</pubDate><category>technical-debt</category><category>software-engineering</category><category>biotech</category><category>knowledge-management</category><category>bioinformatics</category></item><item><title>The Self-Service Data Paradox</title><link>https://boston-wib.org/blog/quicktake/the-self-service-data-paradox</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/the-self-service-data-paradox</guid><description>&lt;p&gt;&lt;em&gt;Why do good tools create more questions than they answer?&lt;/em&gt;&lt;/p&gt;

**ğ—§ğ—µğ—² ğ—ºğ—¼ğ—¿ğ—² ğ˜€ğ—²ğ—¹ğ—³-ğ˜€ğ—²ğ—¿ğ˜ƒğ—¶ğ—°ğ—² ğ˜ğ—¼ğ—¼ğ—¹ğ˜€ ğ˜†ğ—¼ğ˜‚ ğ—¯ğ˜‚ğ—¶ğ—¹ğ—±, ğ˜ğ—µğ—² ğ—ºğ—¼ğ—¿ğ—² ğ—¾ğ˜‚ğ—²ğ˜€ğ˜ğ—¶ğ—¼ğ—»ğ˜€ ğ—½ğ—²ğ—¼ğ—½ğ—¹ğ—² ğ—®ğ˜€ğ—¸.**

This isn&apos;t a bugâ€”it&apos;s a feature. When scientists can independently explore their data, they discover patterns they never knew existed.

â†’ Simple dashboards lead to &quot;Can I filter by this other variable?&quot;

â†’ Basic visualizations spark &quot;What if we overlay this dataset?&quot;

â†’ Quick analyses become &quot;Can we automate this for the whole pipeline?&quot;

#### ğ—§ğ—µğ—² ğ—¨ğ—»ğ—²ğ˜…ğ—½ğ—²ğ—°ğ˜ğ—²ğ—± ğ—–ğ—¼ğ—»ğ˜€ğ—²ğ—¾ğ˜‚ğ—²ğ—»ğ—°ğ—²: ğ—¢ğ—¿ğ—´ğ—®ğ—»ğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ—®ğ—¹ ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ—®ğ˜ğ—¶ğ—¼ğ—» ğŸš€

Suddenly, it&apos;s not just the data team looking at results. Now you have:

â†’ Wet lab scientists spotting metadata inconsistencies

â†’ Project managers identifying workflow bottlenecks

â†’ Business stakeholders asking strategic questions about resource allocation

â†’ QC teams catching labeling errors that would have slipped through manual review

#### ğ—§ğ—µğ—² ğ—›ğ—¶ğ˜ƒğ—² ğ— ğ—¶ğ—»ğ—± ğ—˜ğ—³ğ—³ğ—²ğ—°ğ˜

This distributed access creates something powerful: organizational collective intelligence. Different backgrounds bring different perspectives to the same data.

The computational biologist sees algorithmic patterns. The bench scientist notices experimental artifacts. The project manager spots resource trends. The quality team catches systematic errors.

Each viewpoint validates and enriches the others. Data quality improves not through more rigorous processes, but through more eyes on the problem.

#### ğ——ğ—²ğ˜€ğ—¶ğ—´ğ—»ğ—¶ğ—»ğ—´ ğ—³ğ—¼ğ—¿ ğ˜ğ—µğ—² ğ—£ğ—®ğ—¿ğ—®ğ—±ğ—¼ğ˜…

The key insight: Design for the questions you&apos;ll create, not just the ones you&apos;re solving today.

â†’ Build flexibility into your data models from day one

â†’ Plan for cross-departmental access patterns you haven&apos;t imagined yet

â†’ Create interfaces that grow with user sophistication

â†’ Establish feedback loops that capture emerging use cases

#### ğ—§ğ—µğ—² ğ—•ğ—¿ğ—¶ğ—±ğ—´ğ—² ğ—•ğ˜‚ğ—¶ğ—¹ğ—±ğ—²ğ—¿&apos;ğ˜€ ğ—£ğ—²ğ—¿ğ˜€ğ—½ğ—²ğ—°ğ˜ğ—¶ğ˜ƒğ—²

The self-service paradox taught me that successful data democratization isn&apos;t about reducing questionsâ€”it&apos;s about enabling better questions. When you build tools that empower non-technical users to explore data independently, you&apos;re not just solving today&apos;s analysis bottleneck. You&apos;re unleashing organizational curiosity.

The most successful data infrastructure projects I&apos;ve led weren&apos;t the ones that answered all questions. They were the ones that helped teams ask questions they never knew they needed to answer.

What&apos;s the most unexpected question your self-service tools have generated? How has democratizing data access changed the conversations happening in your organization?</description><pubDate>Mon, 15 Sep 2025 00:00:00 GMT</pubDate><category>data-democratization</category><category>bioinformatics-leadership</category><category>data-strategy</category><category>self-service-analytics</category></item><item><title>Managing Up Isn&apos;t About Office Politics</title><link>https://boston-wib.org/blog/quicktake/managing-up-isnt-about-office-politics</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/managing-up-isnt-about-office-politics</guid><description>&lt;p&gt;&lt;em&gt;Understanding what success looks like from your managerâ€™s perspective is key to advancing your career.&lt;/em&gt;&lt;/p&gt;

Your manager assigns you a project. You execute it perfectly. They seem... underwhelmed.

Sound familiar?

Managing up isn&apos;t schmoozingâ€”it&apos;s understanding what success looks like from your manager&apos;s perspective.

It&apos;s Stakeholder Management in Reverse

Figure out your manager&apos;s goals, then align your work accordingly. Their success directly impacts your opportunities.

What Your Manager Actually Cares About:

- Looking competent to their own manager
- Meeting team metrics
- Reducing their workload and stress
- Having reliable people they can count on
- Avoiding surprises

The Key Question: &quot;How does my work make my manager successful?&quot;

What This Looks Like:

Instead of: &quot;I finished the analysis you asked for.&quot;
Try: &quot;I finished the analysis. Here&apos;s what it shows about our Q4 pipeline, and how I&apos;d present it to leadership.&quot;

Instead of: &quot;The project is delayed because of data issues.&quot;
Try: &quot;I identified data quality issues. Here are three options with trade-offs and my recommendation.&quot;

The Bottom Line

When your manager looks good, they have more influence to get you resources, advocate for promotions, and trust you with bigger projects.

You&apos;re creating a partnership where both of you are more successful together than apart.

What strategies have helped you build effective relationships with your managers?</description><pubDate>Thu, 11 Sep 2025 00:00:00 GMT</pubDate><category>technical-leadership</category><category>managing-up</category><category>career-development</category><category>leadership</category></item><item><title>Press Play on Bioinformatics: Snakemake in Action</title><link>https://boston-wib.org/blog/quicktake/press-play-on-bioinformatics-snakemake-in-action</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/press-play-on-bioinformatics-snakemake-in-action</guid><description>&lt;p&gt;&lt;em&gt;Ever wish running a bioinformatics pipeline was as easy as pressing â€œplayâ€? With Snakemake, it can be.&lt;/em&gt;&lt;/p&gt;

ğŸ Ever wish running a bioinformatics pipeline was as easy as pressing â€œplayâ€?

Thatâ€™s basically what [Snakemake](https://snakemake.readthedocs.io/en/stable/) does!

Think of it like a recipe book for data analysis:

- You list your ingredients (raw data)
- Write down each step (rules and scripts for preprocessing, analysis, plots)
- Hit â€œgoâ€ and it automatically cooks the entire meal for you!

The magic?

âœ… No more re-running everything when just one step changes

âœ… Works on your laptop or scales up to an HPC cluster

âœ… Makes your analysis reproducible, so six months later (or on someone elseâ€™s machine) you get the same results

To put this into practice, I recently built a Snakemake workflow for cervical cancer gene expression analysis.

It:

ğŸ”¹ Fetches data directly from GEO

ğŸ”¹ Runs preprocessing + differential expression analysis

ğŸ”¹ Generates a volcano plot for quick visualization

You basically write a Snakefile describing each step (preprocessing, analysis, visualization), and then run just one command:

```
snakemake --cores 4
```

Thatâ€™s it! Snakemake figures out the order of tasks, runs only whatâ€™s needed, and makes sure results are reproducible.

âœ¨ The BEST part? With the config file updated for your dataset, ANYONE can reproduce the full analysis with just that one command!

ğŸŸ¢ Iâ€™ve shared the pipeline on GitHub here ğŸ‘‰ https://lnkd.in/eS6G7W75

If youâ€™re curious about Snakemake or just want to peek at a reproducible cancer genomics workflow, check it out!</description><pubDate>Tue, 09 Sep 2025 00:00:00 GMT</pubDate><category>bioinformatics</category><category>snakemake</category><category>reproducibility</category><category>data-science</category><category>r-programming</category><category>python</category><category>version-control</category><category>computationalbiology</category><category>learn-by-doing</category><category>women-in-stem</category></item><item><title>Team Building Isn&apos;t About Trust Falls</title><link>https://boston-wib.org/blog/quicktake/team-building-isnt-about-trust-falls</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/team-building-isnt-about-trust-falls</guid><description>&lt;p&gt;&lt;em&gt;The best technical leaders figure out what each team member wants from the projectâ€”then find ways to deliver it.&lt;/em&gt;&lt;/p&gt;

Your team isn&apos;t just a collection of skills. They&apos;re people with goals, ambitions, and things that get them excited about Monday mornings.

The best technical leaders figure out what each team member wants from the projectâ€”then find ways to deliver it. ğŸ¯

**ğ—§ğ—µğ—² ğ—–ğ—¼ğ—»ğ—»ğ—²ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ˜ğ—¼ ğ—¦ğ˜ğ—®ğ—¸ğ—²ğ—µğ—¼ğ—¹ğ—±ğ—²ğ—¿ ğ— ğ—®ğ—»ğ—®ğ—´ğ—²ğ—ºğ—²ğ—»ğ˜**

Remember stakeholder management? Team building follows the same principle: everyone wants something out of the project.

The difference is that with your team, you have more flexibility to actually deliver what motivates them.

**ğ—§ğ—µğ—² ğ— ğ—¼ğ˜ğ—¶ğ˜ƒğ—®ğ˜ğ—¶ğ—¼ğ—» ğ— ğ—®ğ—½**

Instead of assuming what drives your team members, ask:

ğŸ¯ The Skill Builder: &quot;I want to learn Python/machine learning/cloud architecture&quot; â†’ Can you carve out a project component that lets them practice?

ğŸ† The Expert: &quot;I&apos;m really good at data visualization and enjoy it&quot; â†’ Let them own that domain (but check if they want to try something new too)

ğŸš€ The Impact Seeker: &quot;I want to see my work make a real difference&quot; â†’ Connect their contributions to user outcomes and company goals

ğŸ§© The Problem Solver: &quot;I love tackling complex technical challenges&quot; â†’ Give them the gnarliest problems and the autonomy to solve them

**ğ—§ğ—µğ—² ğ——ğ—®ğ—»ğ—´ğ—²ğ—¿ğ—¼ğ˜‚ğ˜€ ğ—”ğ˜€ğ˜€ğ˜‚ğ—ºğ—½ğ˜ğ—¶ğ—¼ğ—»**

Just because someone is good at something doesn&apos;t mean they want to keep doing it.

Your best data engineer might be craving a chance to work on front-end development. Your visualization expert might want to try their hand at backend systems.

Always ask: &quot;You&apos;re great at X. Do you want to keep working on X, or try something different this time?&quot;

**ğ—ªğ—µğ—²ğ—¿ğ—² ğ—¬ğ—¼ğ˜‚ğ—¿ ğ—¦ğ—¼ğ—³ğ˜ ğ—¦ğ—¸ğ—¶ğ—¹ğ—¹ğ˜€ ğ—¦ğ—µğ—¶ğ—»ğ—²**

Team building is where your listening and empathy skills become superpowers:

**ğ—Ÿğ—¶ğ˜€ğ˜ğ—²ğ—» ğ—³ğ—¼ğ—¿ ğ—²ğ—»ğ—²ğ—¿ğ—´ğ˜†**: What makes their voice change when they talk about work?
**ğ—ªğ—®ğ˜ğ—°ğ—µ ğ—³ğ—¼ğ—¿ ğ—²ğ—»ğ—´ğ—®ğ—´ğ—²ğ—ºğ—²ğ—»ğ˜**: What tasks do they dive into vs. drag their feet on?
**ğ—”ğ˜€ğ—¸ ğ—®ğ—¯ğ—¼ğ˜‚ğ˜ ğ—´ğ—¿ğ—¼ğ˜„ğ˜ğ—µ**: Where do they want to be in six months?
**ğ—–ğ—¿ğ—²ğ—®ğ˜ğ—² ğ—¼ğ—½ğ—½ğ—¼ğ—¿ğ˜ğ˜‚ğ—»ğ—¶ğ˜ğ—¶ğ—²ğ˜€**: Can you adjust project scope to include their interests?

**ğ—§ğ—µğ—² ğ—–ğ—¼ğ—ºğ—½ğ—¼ğ˜‚ğ—»ğ—± ğ—˜ğ—³ğ—³ğ—²ğ—°ğ˜**

When team members get what they want from the project:
â†’ They&apos;re more engaged and productive
â†’ They develop new skills that benefit future projects
â†’ They feel valued and stay longer
â†’ They become advocates for your leadership style

You&apos;re not just building a projectâ€”you&apos;re building careers and loyalty.

**ğ—§ğ—µğ—² ğ—•ğ—¿ğ—¶ğ—±ğ—´ğ—² ğ—•ğ˜‚ğ—¶ğ—¹ğ—±ğ—²ğ—¿ ğ—”ğ—½ğ—½ğ—¿ğ—¼ğ—®ğ—°ğ—µ**

Great team building is about connecting individual aspirations to project success. You&apos;re finding the intersection of &quot;what needs to get done&quot; and &quot;what people want to learn/do.&quot;

When those align, work doesn&apos;t feel like work.

What&apos;s your experience with team motivation? How do you discover what really drives your team members?</description><pubDate>Mon, 08 Sep 2025 00:00:00 GMT</pubDate><category>technicalLeadership</category><category>team-building</category><category>career-development</category><category>leadership</category><category>mentorship</category></item><item><title>The Art of Saying No Without Losing Friends</title><link>https://boston-wib.org/blog/quicktake/the-art-of-saying-no-without-losing-friends</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/the-art-of-saying-no-without-losing-friends</guid><description>&lt;p&gt;&lt;em&gt;Scope negotiation isn&apos;t about being rigidâ€”it&apos;s about being intentional.&lt;/em&gt;&lt;/p&gt;

Three weeks into your project, someone says: &quot;Wouldn&apos;t it be nice if we could also...&quot;

Your heart sinks. You know exactly where this leads. ğŸš¨

Scope creep is the silent killer of technical projects. Here&apos;s how to prevent it:

**ğ—¦ğ˜ğ—²ğ—½ ğŸ­: ğ——ğ—²ğ—³ğ—¶ğ—»ğ—² ğ—ªğ—µğ—®ğ˜ ğ—¬ğ—¼ğ˜‚&apos;ğ—¿ğ—² ğ—•ğ˜‚ğ—¶ğ—¹ğ—±ğ—¶ğ—»ğ—´ (ğ—®ğ—»ğ—± ğ—ªğ—µğ—®ğ˜ ğ—¬ğ—¼ğ˜‚&apos;ğ—¿ğ—² ğ—¡ğ—¼ğ˜)**

Start with clear requirements. But here&apos;s the secret: also create an explicit &quot;Out of Scope&quot; section.

Those nice-to-have features people mention in meetings? Write them down as &quot;Version 2.0 features.&quot; Acknowledge them, document them, but don&apos;t build them.

This serves two purposes:
â†’ Shows you&apos;re listening to stakeholder ideas
â†’ Creates a clear boundary for current work

**ğ—¦ğ˜ğ—²ğ—½ ğŸ®: ğ—šğ—²ğ˜ ğ—¦ğ˜ğ—®ğ—¸ğ—²ğ—µğ—¼ğ—¹ğ—±ğ—²ğ—¿ ğ—•ğ˜‚ğ˜†-ğ—¶ğ—»**

Remember your stakeholder mapping? Make sure your project plan has something that satisfies each key player.

Get them to explicitly agree to the scope. Don&apos;t just send a documentâ€”have a conversation.

&quot;This is what we&apos;re building. This is what we&apos;re not building right now. Are we aligned?&quot;

Get verbal confirmation. Even better, get it in writing.

**ğ—¦ğ˜ğ—²ğ—½ ğŸ¯: ğ—¦ğ˜ğ—®ğ˜† ğ—¼ğ—» ğ˜ğ—µğ—² ğ—£ğ—®ğ˜ğ—µ**

Here comes the hard part: during implementation, stick to the plan.

When someone inevitably says &quot;wouldn&apos;t it be nice if...&quot; your response is:
&quot;That&apos;s a great idea! I&apos;m adding it to our Version 2.0 list. Let&apos;s make sure we nail Version 1.0 first.&quot;

**ğ—§ğ—µğ—² ğ—£ğ˜€ğ˜†ğ—°ğ—µğ—¼ğ—¹ğ—¼ğ—´ğ˜† ğ—¼ğ—³ ğ—¦ğ—°ğ—¼ğ—½ğ—² ğ—–ğ—¿ğ—²ğ—²ğ—½**

Why do stakeholders keep adding features mid-project?
â†’ They get excited seeing progress
â†’ They think &quot;just one more thing&quot; won&apos;t hurt
â†’ They don&apos;t understand the compounding complexity
â†’ They haven&apos;t felt the pain of scope creep before

Your job is to protect them from themselves while keeping them engaged.

**ğ— ğ˜† ğ—šğ—¼-ğ—§ğ—¼ ğ—¥ğ—²ğ˜€ğ—½ğ—¼ğ—»ğ˜€ğ—²**:

&quot;I love the enthusiasm! That feature would definitely add value. Here&apos;s what it would cost us in terms of timeline and current scope. Should we adjust our priorities?&quot;

This does three things:
â†’ Acknowledges their idea positively
â†’ Makes the trade-off explicit
â†’ Puts the decision back in their hands

**ğ—§ğ—µğ—² ğ—•ğ—¿ğ—¶ğ—±ğ—´ğ—² ğ—•ğ˜‚ğ—¶ğ—¹ğ—±ğ—²ğ—¿ ğ—œğ—»ğ˜€ğ—¶ğ—´ğ—µğ˜**:

Scope negotiation isn&apos;t about being rigidâ€”it&apos;s about being intentional. You&apos;re helping stakeholders make informed decisions about trade-offs rather than just saying no.

When you frame scope management as protecting shared success rather than limiting possibilities, stakeholders become your allies in maintaining focus.

What&apos;s your experience with scope creep? What strategies help you keep projects on track?</description><pubDate>Thu, 04 Sep 2025 00:00:00 GMT</pubDate><category>technical-leadership</category><category>scope-management</category><category>project-management</category><category>leadership</category><category>career-development</category></item><item><title>A Coffee with CompBio: The Spatial Transcriptomics Toolkit</title><link>https://boston-wib.org/blog/coffeewithcompbio/episode-7</link><guid isPermaLink="true">https://boston-wib.org/blog/coffeewithcompbio/episode-7</guid><description>&lt;p&gt;&lt;em&gt;Memory, Clustering, and Deconvolution&lt;/em&gt;&lt;/p&gt;

import { FaBell, FaSpotify, FaApple } from &apos;react-icons/fa&apos;;

&lt;iframe
  name=&quot;Ausha Podcast Player&quot;
  frameborder=&quot;0&quot;
  loading=&quot;lazy&quot;
  id=&quot;ausha-U2XY&quot;
  height=&quot;220&quot;
  style=&quot;border: none; width:100%; height:220px&quot;
  src=&quot;https://player.ausha.co/?podcastId=pZqqYHGLPVkY&amp;playlist=false&amp;color=%23751cbf&amp;v=3&amp;playerId=ausha-U2XY&quot;
&gt;&lt;/iframe&gt;
&lt;script src=&quot;https://player.ausha.co/ausha-player.js&quot;&gt;&lt;/script&gt;

Alex Barlett and Lorena Pantano tackle the computational challenges of spatial transcriptomics. Learn how **ğ—•ğ—£ğ—–ğ—²ğ—¹ğ—¹ğ˜€** can help you work with millions of cells without needing terabytes of RAM, discover how **ğ—•ğ—®ğ—»ğ—¸ğ˜€ğ˜†&apos;ğ˜€** neighborhood-aware clustering reveals tissue architecture, and explore **ğ—¥ğ—–ğ—§ğ——&apos;ğ˜€** approach to cell type deconvolution in spatially-resolved data. Plus, Lorena reviews **ğ—£ğ—¼ğ˜€ğ—¶ğ˜ğ—¿ğ—¼ğ—»**, the new R-friendly IDE that&apos;s catching attention in the bioinformatics community.

https://lnkd.in/eqFfkzKq

https://lnkd.in/ekrS4H5p

https://lnkd.in/e7r33PKf

https://lnkd.in/eiWqVjjK

https://lnkd.in/e7B35A7V

https://lnkd.in/eZdQ-qKV - Sean Davis

https://positron.posit.co/

Please get in touch if you or your business would like to help support this podcast. Thanks [Amulya Shastr](https://www.linkedin.com/in/amulya-shastry/) for editing and management support.

Listen to this podcast on other platforms:

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaApple size={32} className=&quot;text-black dark:text-white&quot; /&gt;{&apos; &apos;}
    &lt;a href=&quot;https://podcasts.apple.com/us/podcast/the-spatial-transcriptomics-toolkit-memory-clustering/id1817024741?i=1000724623758&quot;&gt;
      Apple
    &lt;/a&gt;
  &lt;/span&gt;

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaSpotify size={32} color=&quot;#1DB954&quot; /&gt;{&apos; &apos;}
    &lt;a href=&quot;https://open.spotify.com/episode/5aTQqIp6Ig0SE27Y5Dzcne?si=ef57ff0893c34321&quot;&gt;Spotify&lt;/a&gt;
  &lt;/span&gt;

Send us your comments, questions, and suggestions using [this form](https://lnkd.in/eJ-hChm3)

We are looking for sponsors! Please get in touch if you or your business would like to help support this podcast.

Follow [Lorena](https://www.linkedin.com/in/lpantano) and [Alex](https://www.linkedin.com/in/alexandra-bartlett-926b32109) on LinkedIn!

If you enjoyed the episode, [please subscribe and leave a review!](https://lnkd.in/eXQ-HpUV)

Hosted by Ausha. See [ausha.co/privacy-policy](https://ausha.co/privacy-policy) for more information</description><pubDate>Tue, 02 Sep 2025 08:00:00 GMT</pubDate><category>career-transition</category><category>bioinformatics</category><category>product-management</category><category>career-development</category><category>from-lab-to-tech</category><category>spatial-transcriptomics</category></item><item><title>How to Turn Stakeholders from Obstacles into Advocates</title><link>https://boston-wib.org/blog/quicktake/how-to-turn-stakeholders-from-obstacles-into-advocates</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/how-to-turn-stakeholders-from-obstacles-into-advocates</guid><description>&lt;p&gt;&lt;em&gt;Technical skills get you the job. Stakeholder management skills make you effective in the job.&lt;/em&gt;&lt;/p&gt;

You just got assigned a new project. Your first instinct? Dive into the technical requirements.

Wrong move. ğŸš«

Your first step should be mapping your stakeholdersâ€”not just who they are, but what makes them tick.

**ğ—¦ğ˜ğ—²ğ—½ ğŸ­: ğ—œğ—±ğ—²ğ—»ğ˜ğ—¶ğ—³ğ˜† ğ—¬ğ—¼ğ˜‚ğ—¿ ğ—¦ğ˜ğ—®ğ—¸ğ—²ğ—µğ—¼ğ—¹ğ—±ğ—²ğ—¿ğ˜€**
Beyond the obvious project sponsor, look for:
â†’ The person whose data you&apos;ll need
â†’ The scientist who&apos;ll actually use what you build
â†’ The team lead who controls resources
â†’ The domain expert who can make or break adoption
â†’ The person who&apos;ll maintain this after you move on

**ğ—¦ğ˜ğ—²ğ—½ ğŸ®: ğ—¨ğ—»ğ—±ğ—²ğ—¿ğ˜€ğ˜ğ—®ğ—»ğ—± ğ—ªğ—µğ—®ğ˜ ğ——ğ—¿ğ—¶ğ˜ƒğ—²ğ˜€ ğ—§ğ—µğ—²ğ—º**
Each stakeholder has different motivations:

ğŸ¤ The Collaborator: wants shared wins and team recognition.
ğŸ¯ The Individualist: wants personal impact and clear attribution.
ğŸ§© The Problem-Solver: wants intellectual challenge.
ğŸ“Š The Results-Oriented person: wants measurable outcomes and clear timelines.

**ğ—¦ğ˜ğ—²ğ—½ ğŸ¯: ğ—™ğ—¶ğ—»ğ—± ğ˜ğ—µğ—² ğ—¢ğ˜ƒğ—²ğ—¿ğ—¹ğ—®ğ—½**
Here&apos;s where you build the bridges: connect project components to stakeholder strengths and motivations.

Data validation â†’ Problem-solver designs checks
User interfaces â†’ Collaborator gathers feedback
Leadership buy-in â†’ Results person frames metrics

**ğ—¦ğ˜ğ—²ğ—½ ğŸ°: ğ— ğ—®ğ—¸ğ—² ğ—§ğ—µğ—²ğ—º ğ—Ÿğ—¼ğ—¼ğ—¸ ğ—šğ—¼ğ—¼ğ—±**
Make every interaction help stakeholders succeed in their roles.

Expert prevents pitfall? Recognize their insight.
Owner provides clean data? Highlight their contribution.

**ğ—§ğ—µğ—² ğ—•ğ—¿ğ—¶ğ—±ğ—´ğ—² ğ—•ğ˜‚ğ—¶ğ—¹ğ—±ğ—²ğ—¿ ğ—œğ—»ğ˜€ğ—¶ğ—´ğ—µğ˜**:
Stakeholder management is alignment, not manipulation.
When they feel ownership, they become advocates instead of obstacles. âœ¨

**ğ—ªğ—µğ—®ğ˜ ğ—§ğ—µğ—¶ğ˜€ ğ—Ÿğ—¼ğ—¼ğ—¸ğ˜€ ğ—Ÿğ—¶ğ—¸ğ—²**:
Instead of: &quot;I need access to the production database.&quot;
Try: &quot;I&apos;d love your input on the safest way to access the data we need. What would make you comfortable with our approach?&quot;
Instead of: &quot;The analysis is taking longer than expected.&quot;
Try: &quot;We discovered some interesting data quality patterns that might impact future projects. Can we schedule time to discuss what we learned?&quot;

**ğ—§ğ—µğ—² ğ—•ğ—¼ğ˜ğ˜ğ—¼ğ—º ğ—Ÿğ—¶ğ—»ğ—²**:
Technical skills get you the job. Stakeholder management skills make you effective in the job.
Your project&apos;s success depends more on people dynamics than code quality. Plan accordingly. ğŸ¯

What&apos;s your experience with stakeholder management? What strategies have worked (or failed) for you?</description><pubDate>Tue, 02 Sep 2025 00:00:00 GMT</pubDate><category>technical-leadership</category><category>stakeholder-management</category><category>project-management</category><category>career-development</category><category>leadership</category></item><item><title>The Genomics Diversity Crisis</title><link>https://boston-wib.org/blog/deepdive/deiGenomics</link><guid isPermaLink="true">https://boston-wib.org/blog/deepdive/deiGenomics</guid><description>&lt;p&gt;&lt;em&gt;When 86% of genomic data comes from European ancestry, treatments built on this data will inevitably fail marginalized communities.&lt;/em&gt;&lt;/p&gt;

As diversity, equity, and inclusion (DEI) initiatives are being dismantled across US institutions, Eric Green, former Director of the National Human Genome Research Institute, opened the Festival of Genomics in Boston to make a case that DEI extends far beyond creating a better workplace culture. In genomics research, DEI is scientifically essential. DEI refers to efforts that ensure diverse representation (diversity) through fair treatment and opportunity (equity) and meaningful participation (inclusion), all grounded in respect for different communities and perspectives. Current genomic datasets overwhelmingly represent people of European ancestry, yet the insights derived from this narrow slice of humanity are being applied to diagnose, treat, and understand disease across all populations. To truly reflect human diversity, science depends on data from all communities. However, that data cannot be collected from populations who distrust the scientific establishment. Genuine commitments to equity and inclusion are the foundation needed to rebuild those critical relationships.

### The Technical Problem: Sampling Bias

Sampling bias is a fundamental challenge at the heart of genomics research. This occurs when data used to build a model fails to adequately represent the study or target population due to the underrepresentation of certain groups. A classic example of sampling bias would be trying to understand human height by collecting data only from NBA players. The resulting model would drastically overestimate how tall humans are. Since sampling bias compromises generalizability, these models often produce misleading outputs. In genomics, sampling bias can lead researchers to overestimate or underestimate impacts of genetic variants or treatments.

In the field of genomics, the risk of sampling bias takes on particular urgency. In 2021, an estimated 86% of sequenced genomic data came from individuals of European ancestry [^1]. This staggering imbalance means that models trained on this data are systematically misleading and, therefore, unreliable across diverse populations. This is a profound problem because models that fail to generalize to marginalized communities will inevitably exacerbate existing health disparities.

As bioinformatics enters a new era of artificial intelligence (AI) driven discovery, the composition of our training data has never mattered more. How can we build a future of personalized medicine on a foundation that only represents less than 20% of the world&apos;s population?

### The Human Problem: A Legacy of Distrust

To improve our biological models, we must prioritize diverse dataset collection. However, while the solution seems straightforward, the reality is far more complex. The scientific community&apos;s painful history of human exploitation and data misuse continues to stifle many communities&apos; willingness to participate in research studies.

These scars run deep. From 1932 to 1972, with support from state and local governments, the US Public Health Service conducted what became known as the Tuskegee Syphilis Study, misleading impoverished Black male sharecroppers to participate in a treatment program for their &quot;bad blood&quot; [^2], [^3]. In reality, the program was a study of the progression of untreated syphilis. Withheld from diagnoses and available treatment, hundreds of participants lost their lives from the disease in the name of scientific advancement.

Even well-intentioned research can have ethical missteps that deepen distrust. In the 1990s, as the rate of diabetes climbed in the Havasupai tribe, an indigenous community near the Grand Canyon, around 650 members donated blood samples to Arizona State University to study genetic links to diabetes [^4]. Approximately a hundred participants signed a consent form allowing their samples to be used to &quot;study the causes of behavior/medical disorders.&quot; However, with English as a second language for many participants and most having never completed high school, the full implications of this broad consent were at high risk of not being understood. Therefore, when researchers used the tribe&apos;s samples in studies unrelated to diabetes, it was done with disregard to the civil rights of the Havasupai tribe to self-determination. Rightfully, the Havasupai tribe sued the university and has refused to participate in any further studies.

More recently, the rise of consumer genetic testing, such as those offered by 23andMe, has made it easy to share genetic data. However, these platforms have also raised concerns about data transparency for marginalized communities [^5]. These companies tell consumers that consumers own their personal data. However, for communities who are already wary of government surveillance and over-policing, the knowledge that their genetic information could potentially be sold to pharmaceutical companies to develop medicine that they will not have access to or accessed by law enforcement in ways that could lead to wrongful convictions for them or a family member adds another layer of hesitation to research participation.

The examples listed only touch the surface of these deeply rooted issues that plague distrust of the scientific community, and I encourage readers to learn more outside the context of this article.

### A Path Forward: DEI as a Framework for Rebuilding Trust

History shows that trust in science, even after profound breaches, can be rebuilt. The atrocities conducted by medical professionals in the Holocaust ruptured the relationship between science and Jewish communities [^6]. Despite this history, today the Ashkenazi Jewish population is among the most studied groups in human genomics. This reconciliation was made possible through decades of ethical reform and intentional efforts to include and support Jewish scientists.

The Nuremberg Code, a framework established in response to unethical Nazi medical experiments, set up crucial protections for research participants through informed consent and participant autonomy [^7]. However, while these protections support equitable practices, they did not insure participants have meaningful representation and decision-making power in the research itself. For the Jewish community, this gap was filled by inclusive practices like the Rockefeller Foundation&apos;s Refugee Scholar Program, which resettled Jewish scientists and supported their continued involvement in research despite widespread persecution [^8]. This combination of ethical frameworks and institutional inclusion likely contributed significantly to rebuilding trust between Jewish communities and scientific institutions.

Until recently, we were seeing similar patterns emerging through initiatives of inclusion. Researchers from communities affected by HIV/AIDS has been associated with improved rates of community engagement in research and better treatment outcomes [^9]. Similarly, Indigenous health research programs indicate that studies of their communities benefit from Indigenous leadership by increasing community engagement and fostering resources to support non-Indigenous research team members to develop cultural competency [^10]. These studies ensured that voices of the communities they were doing research on were empowered to shape the research process, from study design to data interpretation.

It must be emphasized that practicing DEI is not a quick fix. DEI practices must be sustained, proactive commitments to ensure diversity through informed consent and inclusion. As genomics continues to evolve, so must our standards for how research is conducted, whose data is included, and how the data is used. By building trust, we build better science and, in turn, better health outcomes for everyone.

As we stand at the threshold of AI-driven genomics, we have a choice to make. We can continue building models on a narrow foundation that serves only a fraction of humanity, or we can invest in the trust-building work necessary to create truly inclusive research. The technical quality of our science depends on our decision.

### References:

[^1]: Fatumo, S., Chikowore, T., Choudhury, A., Ayub, M., Martin, A. R., &amp; KuchenbÃ¤cker, K. (2022). Diversity in genomic studies: a roadmap to address the imbalance. Nature medicine, 28(2), 243.

[^2]: Jones, J. H. (1993). Bad blood: the Tuskegee syphilis experiment. New and expanded ed. New York.

[^3]: Gray, F. (1998). The Tuskegee syphilis study: An insiderâ€™s account.Â Montgomery, AL: Black Belt.

[^4]: Sterling, R. L. (2011). Genetic research among the Havasupai: a cautionary tale. AMA Journal of Ethics, 13(2), 113-117.

[^5]: Raz, A. E., Niemiec, E., Howard, H. C., Sterckx, S., Cockbain, J., &amp; Prainsack, B. (2020). Transparency, consent and trust in the use of customers&apos; data by an online genetic testing company: an exploratory survey among 23andMe users.Â New Genetics and Society,Â 39(4), 459-482.

[^6]: Lagnado, L. M., &amp; Dekel, S. C. (1992).Â Children of the flames: Dr. Josef Mengele and the untold story of the twins of Auschwitz. Penguin.

[^7]: Trials of War Criminals before the Nuremberg Military Tribunals under Control Council Law No. 10. (n.d.). Permissible medical experiments. (Vol. 2, pp. 181-182). Washington, D.C.: U.S. Government Printing Office.

[^8]: Iacobelli, T. (2021). The Rockefeller Foundationâ€™s Refugee Scholar Program.

[^9]: Karris, M. Y., Dube, K., &amp; Moore, A. A. (2020). What lessons it might teach us? Community engagement in HIV research. Current Opinion in HIV and AIDS, 15(2), 142-149.

[^10]: Woods, C., Settee, C., Beaucage, M., Robinson-Settee, H., Desjarlais, A., Adams, E., ... &amp; Nahanee, D. (2023). Ensuring Indigenous co-leadership in health research: a Can-SOLVE CKD case example. International Journal for Equity in Health, 22(1), 234.</description><pubDate>Mon, 01 Sep 2025 14:00:00 GMT</pubDate><category>diversity-equity-inclusion</category><category>dei-in-science</category><category>inclusive-research</category><category>genetics-research</category><category>equity-in-science</category><category>diversity-in-stem</category><category>genomics-equity</category></item><item><title>The Engineering Report That Never Gets Written</title><link>https://boston-wib.org/blog/quicktake/engineering-report-that-never-gets-written</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/engineering-report-that-never-gets-written</guid><description>&lt;p&gt;&lt;em&gt;Software engineers routinely write project wrap-up reports. Bioinformaticians? Almost never.&lt;/em&gt;&lt;/p&gt;

You just finished a three-month bioinformatics project. The analysis is done, results delivered, everyone moves on to the next urgent task.

But what about all the things you learned along the way?

Software engineers routinely write project wrap-up reports. Bioinformaticians? Almost never.

THE MISSING KNOWLEDGE:

- Which approaches worked (and which didn&apos;t)
- What data quality issues you discovered
- Where you spent the most debugging time
- What you&apos;d do differently knowing what you know now

All of that hard-won knowledge just... evaporates.

WHY WE DON&apos;T DO IT: The honest reason? Time. Nobody wants to stop and document when there&apos;s another analysis waiting. Leadership isn&apos;t pushing for it.

THE BUSINESS CASE: Your company just invested weeks or months of your time on this project. What if the next person doing similar work could skip the dead-end approaches you already tried and build on your work instead of reinventing it?

Every repeated mistake is time stolen from innovation.

WHAT IT LOOKS LIKE: It doesn&apos;t need to be formal. A Confluence page, slide deck, or simple document:

- Project overview and key technical decisions
- What worked vs. what didn&apos;t
- Lessons learned and recommendations
- Useful resources discovered

MAKING IT HAPPEN:

- Build 2-3 hours of &quot;project wrap-up&quot; into every timeline
- Create a simple template
- Store reports where people can find them
- Make it part of project closure, not an afterthought

**KEY INSIGHT**: Engineering reports aren&apos;t just documentationâ€”they&apos;re knowledge transfer tools. They help teams build on each other&apos;s work instead of starting from scratch every time.

When people leave, does their knowledge walk out the door with them?
What&apos;s your experience with project documentation? Have you seen teams successfully capture and share learnings?</description><pubDate>Mon, 25 Aug 2025 00:00:00 GMT</pubDate><category>bioinformatics</category><category>knowledge-management</category><category>project-management</category><category>engineering-culture</category><category>technical-writing</category></item><item><title>The Post-Mortem No One Wants to Do (But Everyone Should)</title><link>https://boston-wib.org/blog/quicktake/the-post-mortem-no-one-wants-to-do-but-everyone-should</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/the-post-mortem-no-one-wants-to-do-but-everyone-should</guid><description>&lt;p&gt;&lt;em&gt;Thirty minutes you spend on a post-mortem can save you thirty hours of future firefighting.&lt;/em&gt;&lt;/p&gt;

Something breaks. Data pipeline fails. Analysis crashes. Dashboard goes down.

Your first instinct? Fix it fast and move on. Nobody wants to dwell on what went wrong.

But here&apos;s what I&apos;ve learned: the 30 minutes you spend on a post-mortem can save you 30 hours of future firefighting.

THE NATURAL RESPONSE: When things break, we&apos;re frustrated. We want to forget it happened and get back to &quot;real work.&quot; The failure feels like a setback, so we rush to put it behind us.

I get it. Post-mortems feel like dwelling on negative things when you could be building new features.

WHAT POST-MORTEMS ACTUALLY DO:

- Identify systemic issues (not just the immediate bug)
- Reveal process gaps you didn&apos;t know existed
- Prevent the same failure from happening again
- Make your systems more robust by addressing root causes
- Turn failures into learning opportunities for the whole team

A REAL EXAMPLE: Our visualization dashboard went down because a scientist uploaded a malformed CSV. Easy fix: validate the file format.

Post-mortem revealed the real issue: we had no systematic way to communicate data formatting requirements to scientists. The CSV was just the symptom.

Solution: Built an automated data validation step with clear error messages that taught scientists the expected format.

Result: Turned a one-off failure into a system improvement that prevented dozens of future issues.

THE PROCESS THAT WORKS:

1. Designate someone to lead the investigation (don&apos;t let it fall through the cracks)
2. Focus on systems, not blame (what failed, not who failed)
3. Include relevant stakeholders (the people who were affected need to understand what happened)
4. Document actionable improvements (not just &quot;be more careful next time&quot;)

THE MANAGEMENT CHALLENGE: Here&apos;s the tricky part: post-mortems require time that doesn&apos;t immediately show ROI.

If leadership just wants to &quot;move fast and fix things,&quot; it&apos;s hard to justify spending time on &quot;what went wrong&quot; instead of &quot;what&apos;s next.&quot;

But here&apos;s the business case: every failure that repeats is time stolen from innovation. Post-mortems are an investment in not having the same conversation again in three months.

THE BRIDGE BUILDER PERSPECTIVE: Post-mortems aren&apos;t just technical exercisesâ€”they&apos;re communication opportunities. They help teams understand how their work interconnects and where the fragile points are.

When you include stakeholders in post-mortems, you&apos;re not just fixing systemsâ€”you&apos;re building shared understanding of how your infrastructure works and why certain practices matter.

Have you found effective ways to make time for post-mortems? How do you convince leadership that this &quot;backward-looking&quot; work is actually forward-thinking?</description><pubDate>Thu, 21 Aug 2025 00:00:00 GMT</pubDate><category>post-mortem</category><category>process-improvement</category><category>systems-thinking</category><category>technical-leadership</category><category>continuous-improvement</category></item><item><title>The One Question That Changed How I Build Tools</title><link>https://boston-wib.org/blog/quicktake/the-one-question-that-changed-how-i-build-tools</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/the-one-question-that-changed-how-i-build-tools</guid><description>&lt;p&gt;&lt;em&gt;Successful tool development begins with understanding where to draw the finish line.&lt;/em&gt;&lt;/p&gt;

&quot;Can you make a dashboard for our RNA-seq data?&quot;

I used to jump straight into requirements gathering. What data? Which visualizations? How many samples?

Now I ask one question first: &quot;What does success look like?&quot;

THE SAME REQUEST, DIFFERENT SUCCESS CRITERIA:

Scenario: &quot;We need an RNA-seq dashboard&quot;

- If you&apos;re the Lab Manager: Success = &quot;I can quickly spot failed experiments before they waste downstream resources.&quot; â†’ Build: QC-focused dashboard with clear pass/fail indicators

- If you&apos;re the Principal Investigator: Success = &quot;I can confidently present these results to the grant committee next week.&quot; â†’ Build: Publication-ready visualizations with statistical annotations

- If you&apos;re the Postdoc: Success = &quot;I can explore the data myself without bothering the bioinformatics team every time I have a question.&quot; â†’ Build: Interactive exploration tool with multiple filtering options

Same request. Three completely different tools.

WHY REQUIREMENTS AREN&apos;T ENOUGH: Requirements tell you WHAT to build. Success criteria tell you WHY you&apos;re building it.

- &quot;Show differentially expressed genes&quot; is a requirement.
- &quot;Help me identify the top 3 pathways to focus our next experiments on&quot; is a success criterion.

The second one tells you that you need statistical significance, pathway enrichment, and probably some way to rank or prioritize results.

THE POWER OF STARTING WITH OUTCOMES: When you start with success criteria:

- You build tools people actually use
- You avoid feature creep (if it doesn&apos;t serve the success criteria, it&apos;s not essential)
- You can make trade-offs confidently
- You know when you&apos;re done

A REAL EXAMPLE:

Scientist: &quot;I need a way to visualize our compound screening data.&quot;
Me: &quot;What does success look like?&quot;
Scientist: &quot;I want to walk into Monday&apos;s meeting and confidently say &apos;these 5 compounds are worth pursuing&apos; and defend that decision.&quot;

Suddenly I&apos;m not building a generic visualization tool. I&apos;m building a decision-support system with confidence intervals, statistical significance testing, and clear ranking criteria.

**THE BRIDGE BUILDER INSIGHT**: Different stakeholders define success differently, even for identical requests. Your job isn&apos;t just to translate requirements -- it&apos;s to uncover and align success criteria.

Sometimes the real win is realizing that three different stakeholders need three different tools, not one &quot;comprehensive&quot; solution that satisfies nobody.

What&apos;s your experience with this? Do you find that starting with outcomes changes what you build?</description><pubDate>Mon, 18 Aug 2025 00:00:00 GMT</pubDate><category>data-science</category><category>requirements</category><category>user-experience</category><category>bioinformatics</category><category>product-development</category></item><item><title>A Coffee with CompBio: First hand experience on transitioning to a Product Manager role</title><link>https://boston-wib.org/blog/coffeewithcompbio/episode-6</link><guid isPermaLink="true">https://boston-wib.org/blog/coffeewithcompbio/episode-6</guid><description>&lt;p&gt;&lt;em&gt;Katie Huges shares her professional journey from bioinformatics to product management with help from Lorena and Alex.&lt;/em&gt;&lt;/p&gt;

import { FaBell, FaSpotify, FaApple } from &apos;react-icons/fa&apos;;

&lt;iframe
  name=&quot;Ausha Podcast Player&quot;
  frameborder=&quot;0&quot;
  loading=&quot;lazy&quot;
  id=&quot;ausha-O3Nv&quot;
  height=&quot;220&quot;
  style=&quot;border: none; width:100%; height:220px&quot;
  src=&quot;https://player.ausha.co/?podcastId=PqNNvCJ4ZnWd&amp;playlist=false&amp;color=%23751cbf&amp;v=3&amp;playerId=ausha-O3Nv&quot;
&gt;&lt;/iframe&gt;
&lt;script src=&quot;https://player.ausha.co/ausha-player.js&quot;&gt;&lt;/script&gt;

Alex Barlett and Lorena Pantano welcome Katie Hughes, their first guest, to discuss her career transition from bioinformatics to product management. Katie shares her journey from studying genetics, working in wet labs, and discovering a passion for bioinformatics, to eventually earning a master&apos;s degree in the field. She details her experience at various biotech companies, including Harvard Medical School, Moderna, Sonata Therapeutics, and Generate Biomedicines. Katie emphasizes the importance of curiosity, adaptability, and soft skills in making career transitions. She explains what a product manager does, differentiates it from similar roles, and outlines the skills and experiences that helped her succeed. The discussion also covers the day-to-day responsibilities of a product manager, the collaborative nature of the role, and advice for those interested in making a similar career shift.

[Marty Cagan](https://www.svpg.com/books/inspired-how-to-create-tech-products-customers-love-2nd-edition/)

[How I AI podcast](https://youtube.com/@howiaipodcast?si=CYby_n5KrKUKqo2u)

Listen to this podcast on other platforms:

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaApple size={32} className=&quot;text-black dark:text-white&quot; /&gt;{&apos; &apos;}
    &lt;a href=&quot;https://podcasts.apple.com/us/podcast/a-coffee-with-katie-hughes-first-hand-experience/id1817024741?i=1000721700181&quot;&gt;
      Apple
    &lt;/a&gt;
  &lt;/span&gt;

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaSpotify size={32} color=&quot;#1DB954&quot; /&gt;{&apos; &apos;}
    &lt;a href=&quot;https://open.spotify.com/episode/4m4y1iCk1AaVkBOWjnsJ3l?si=d47a019944024d2e&quot;&gt;Spotify&lt;/a&gt;
  &lt;/span&gt;

Send us your comments, questions, and suggestions using [this form](https://lnkd.in/eJ-hChm3)

We are looking for sponsors! Please get in touch if you or your business would like to help support this podcast.

Follow [Lorena](https://www.linkedin.com/in/lpantano) and [Alex](https://www.linkedin.com/in/alexandra-bartlett-926b32109) on LinkedIn!

If you enjoyed the episode, [please subscribe and leave a review!](https://lnkd.in/eXQ-HpUV)

Hosted by Ausha. See [ausha.co/privacy-policy](https://ausha.co/privacy-policy) for more information</description><pubDate>Wed, 13 Aug 2025 00:00:00 GMT</pubDate><category>career-transition</category><category>bioinformatics</category><category>product-management</category><category>career-development</category><category>from-lab-to-tech</category></item><item><title>The Hidden Bottleneck in AI-Driven Drug Discovery</title><link>https://boston-wib.org/blog/quicktake/hidden-bottleneck-ai-drug-discovery</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/hidden-bottleneck-ai-drug-discovery</guid><description>&lt;p&gt;&lt;em&gt;The real challenge in AI-drive drug discovery is the broken data infrastructure.&lt;/em&gt;&lt;/p&gt;

Everyone talks about AI algorithms in drug discovery. The real bottleneck? The data pipelines feeding them.

I spend my days upstream of AI initiatives, and I see the same pattern everywhere: brilliant algorithms starving on broken data infrastructure.

THE SPREADSHEET PROBLEM: Data lives in Excel files emailed between teams. Results shared via Slack attachments. &quot;Final&quot; datasets saved in SharePoint with links passed around like digital hot potatoes.

Sound familiar?

This isn&apos;t just messyâ€”it&apos;s data integrity suicide.

WHAT BREAKS:

- No provenance: Where did this data come from? Which version is current?
- No lineage: How was this dataset processed? Can we reproduce it?
- Human error everywhere: Copy-paste mistakes, version confusion, accidental overwrites
- AI garbage in, garbage out: Your model is only as good as your training data

THE REAL CHALLENGE: You need a framework for where data can live that&apos;s:

- Extensible: Science moves faster than software. Your system needs to adapt.
- Connected: Automated data flow from instruments â†’ LIMS â†’ analysis â†’ notebooks
- Traceable: Every data point has a story you can follow

WHAT THIS LOOKS LIKE: Instead of emailing Excel files, you have:

- Instruments that automatically deposit data into structured systems
- LIMS that captures experimental metadata and sample lineage
- Automated pipelines that connect wet lab data to computational analysis
- Lab notebooks that link directly to the data they reference

THE AI PAYOFF: When your data infrastructure is solid, AI initiatives actually work. Your models train on clean, well-documented data. You can trace every prediction back to its source. You can reproduce and validate results.

When it&apos;s broken? Your data scientists spend 80% of their time hunting for data and questioning its quality.

REALITY CHECK: This is R&amp;D/early discovery perspective. Regulated environments have different (often stricter) requirements. But the principle remains: AI success starts with data infrastructure.

Most biotech companies are trying to solve the algorithm problem when they should be solving the data problem first.

Your AI is only as smart as the data pipeline feeding it. Fix the pipeline, unleash the potential.

What&apos;s your experience with data infrastructure challenges in AI initiatives? Have you seen companies get this balance right?</description><pubDate>Mon, 11 Aug 2025 00:00:00 GMT</pubDate><category>AI</category><category>drug-discovery</category><category>data-infrastructure</category><category>biotech</category><category>data-science</category><category>LIMS</category></item><item><title>The Software Engineering Principle No One Teaches in Bioinformatics</title><link>https://boston-wib.org/blog/quicktake/software-engineering-principle-no-one-teaches-in-bioinformatics</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/software-engineering-principle-no-one-teaches-in-bioinformatics</guid><description>&lt;p&gt;&lt;em&gt;Each part of your code should have one job and do it well.&lt;/em&gt;&lt;/p&gt;

Separation of Concerns - it&apos;s one of the most important concepts in software engineering, and somehow it never made it into my bioinformatics courses ğŸ¤” .

I learned this the hard way when trying to scale prototype analyses into maintainable, production-ready tools.

**THE PROTOTYPE TRAP**: Your initial analysis script works perfectly. It reads data, cleans it, runs analysis, generates plots, and saves results. All in one beautiful 500-line Python script.

Then stakeholders ask: &quot;Can you run this on different data?&quot; &quot;Can we change the visualization?&quot; &quot;What if we use a different algorithm?&quot;
Suddenly, your elegant prototype becomes a maintenance nightmare ğŸ˜µâ€ğŸ’« .

**WHAT IS SEPARATION OF CONCERNS?** Simply put: each part of your code should have one job and do it well.

Instead of one script that does everything, you separate:

- Data ingestion (reading files, databases)
- Data processing (cleaning, transformation, QC)
- Analysis logic (algorithms, statistics)
- Visualization (plotting, reporting)
- Output handling (saving results)

**WHY THIS MATTERS**: Need to swap your RNA-seq aligner? Easyâ€”you only touch the analysis module. Your data cleaning logic works for multiple projects. You can test each component independently.

**NEXTFLOW: SEPARATION OF CONCERNS IN ACTION** Many bioinformaticians already use this principle! NextFlow/Snakemake/CWL/etc workflows are a great example:

- Each process handles one specific task
- Swap your aligner? Only modify that processâ€”the rest stays unchanged

**THE EDUCATION GAP**: Most bioinformatics courses focus on algorithms and statistics (crucial!) but don&apos;t explicitly teach these software engineering principles.

We learn sequence alignment but not why organizing code into modular, single-purpose components makes everything more maintainable.

**FOR PRACTITIONERS**: If your analysis scripts are becoming unmaintainable monsters, it might be time to refactor with separation of concerns in mind.

What software engineering principles do you wish you&apos;d learned earlier in your bioinformatics career?</description><pubDate>Thu, 07 Aug 2025 00:00:00 GMT</pubDate><category>bioinformatics</category><category>data-science</category><category>code-quality</category><category>software-engineering</category><category>computational-biology</category></item><item><title>Why Every Biotech Needs a Data Steward</title><link>https://boston-wib.org/blog/quicktake/why-every-biotech-needs-a-data-steward</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/why-every-biotech-needs-a-data-steward</guid><description>&lt;p&gt;&lt;em&gt;What is a data steward? Why is important?&lt;/em&gt;&lt;/p&gt;

I&apos;ve been doing &quot;data stewardship&quot; work for years without calling it thatâ€”and definitely without getting paid for it.

Setting up naming conventions, documenting data lineage, establishing quality checks, creating metadata standards. I did it because I knew it would save me headaches later when someone asked, &quot;Can you reproduce that analysis from six months ago?&quot;

But here&apos;s the problem: this critical work is always treated as a &quot;side project&quot; that gets squeezed between &quot;real&quot; deliverables.

WHAT HAPPENS WITHOUT A DATA STEWARD:

- Scientists can&apos;t find the data they generated last quarter
- &quot;Quick analyses&quot; take days because nobody knows which dataset is the &quot;clean&quot; version
- Due diligence fails because data provenance is unclear
- Team members leave and take institutional knowledge with them
- The same data quality issues get discovered (and fixed) repeatedly

WHY DATA ENGINEERS AREN&apos;T ENOUGH: Data engineers build pipelines. Data stewards govern what flows through them.

A data engineer might build an ETL process. A data steward ensures that process includes proper metadata capture, establishes what &quot;clean&quot; means for your organization, and creates the documentation that lets someone else maintain it.

WHAT A DATA STEWARD ACTUALLY DOES:

- Establishes and enforces data standards across teams
- Creates the metadata that makes data findable and trustworthy
- Builds governance processes that scale with your organization
- Serves as the bridge between data generators (scientists) and data consumers (everyone)
- Prevents technical debt before it becomes a crisis

THE BUSINESS CASE: How much time does your team spend hunting for data, questioning data quality, or rebuilding analyses because the original isn&apos;t reproducible?

If each scientist spends even 2 hours per week on &quot;data archaeology,&quot; that&apos;s probably enough to justify a full-time steward.
Add in the risk mitigation (failed due diligence, regulatory compliance, patent disputes) and the ROI becomes obvious.

TO BIOTECH LEADERS: This isn&apos;t a &quot;nice to have&quot; role. Data stewardship is infrastructureâ€”invisible when it works, catastrophic when it doesn&apos;t.

You wouldn&apos;t run a lab without quality control processes. Don&apos;t run your data operations without data governance.

The question isn&apos;t whether you can afford a data steward. It&apos;s whether you can afford not to have one.

What&apos;s your experience with data governance in biotech? Have you seen companies invest in dedicated stewardship roles, or is this still &quot;everyone&apos;s job&quot; (which usually means no one&apos;s job)?</description><pubDate>Mon, 04 Aug 2025 00:00:00 GMT</pubDate><category>biotech</category><category>data-governance</category><category>data-stewardship</category><category>data-quality</category><category>techLeadership</category></item><item><title>The Power of Listening Across Teams</title><link>https://boston-wib.org/blog/quicktake/the-power-of-listening-across-teams</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/the-power-of-listening-across-teams</guid><description>&lt;p&gt;&lt;em&gt;Listening skills in bioinformatics is critical&lt;/em&gt;&lt;/p&gt;

I&apos;ve been doing &quot;data stewardship&quot; work for years without calling it thatâ€”and definitely without getting paid for it.

Setting up naming conventions, documenting data lineage, establishing quality checks, creating metadata standards. I did it because I knew it would save me headaches later when someone asked, &quot;Can you reproduce that analysis from six months ago?&quot;

But here&apos;s the problem: this critical work is always treated as a &quot;side project&quot; that gets squeezed between &quot;real&quot; deliverables.

WHAT HAPPENS WITHOUT A DATA STEWARD:

- Scientists can&apos;t find the data they generated last quarter
- &quot;Quick analyses&quot; take days because nobody knows which dataset is the &quot;clean&quot; version
- Due diligence fails because data provenance is unclear
- Team members leave and take institutional knowledge with them
- The same data quality issues get discovered (and fixed) repeatedly

WHY DATA ENGINEERS AREN&apos;T ENOUGH: Data engineers build pipelines. Data stewards govern what flows through them.

A data engineer might build an ETL process. A data steward ensures that process includes proper metadata capture, establishes what &quot;clean&quot; means for your organization, and creates the documentation that lets someone else maintain it.

WHAT A DATA STEWARD ACTUALLY DOES:

- Establishes and enforces data standards across teams
- Creates the metadata that makes data findable and trustworthy
- Builds governance processes that scale with your organization
- Serves as the bridge between data generators (scientists) and data consumers (everyone)
- Prevents technical debt before it becomes a crisis

THE BUSINESS CASE: How much time does your team spend hunting for data, questioning data quality, or rebuilding analyses because the original isn&apos;t reproducible?

If each scientist spends even 2 hours per week on &quot;data archaeology,&quot; that&apos;s probably enough to justify a full-time steward.
Add in the risk mitigation (failed due diligence, regulatory compliance, patent disputes) and the ROI becomes obvious.

TO BIOTECH LEADERS: This isn&apos;t a &quot;nice to have&quot; role. Data stewardship is infrastructureâ€”invisible when it works, catastrophic when it doesn&apos;t.

You wouldn&apos;t run a lab without quality control processes. Don&apos;t run your data operations without data governance.

The question isn&apos;t whether you can afford a data steward. It&apos;s whether you can afford not to have one.

What&apos;s your experience with data governance in biotech? Have you seen companies invest in dedicated stewardship roles, or is this still &quot;everyone&apos;s job&quot; (which usually means no one&apos;s job)?</description><pubDate>Fri, 01 Aug 2025 00:00:00 GMT</pubDate><category>cross-functional</category><category>empathy</category><category>bioinformatics</category><category>teamwork</category><category>technical-leadership</category></item><item><title>A Coffee with CompBio: R You Doing It Right?</title><link>https://boston-wib.org/blog/coffeewithcompbio/episode-5</link><guid isPermaLink="true">https://boston-wib.org/blog/coffeewithcompbio/episode-5</guid><description>&lt;p&gt;&lt;em&gt;Alex and Lorena dig into the tricks and tips that&apos;ll actually make your R code work better.&lt;/em&gt;&lt;/p&gt;

import { FaBell, FaSpotify, FaApple } from &apos;react-icons/fa&apos;;

&lt;iframe
  name=&quot;Ausha Podcast Player&quot;
  frameborder=&quot;0&quot;
  loading=&quot;lazy&quot;
  id=&quot;ausha-lSBx&quot;
  height=&quot;220&quot;
  style=&quot;border: none; width:100%; height:220px&quot;
  src=&quot;https://player.ausha.co/?podcastId=Ljzz7heQJNRj&amp;playlist=false&amp;color=%23751cbf&amp;v=3&amp;playerId=ausha-lSBx&quot;
&gt;&lt;/iframe&gt;
&lt;script src=&quot;https://player.ausha.co/ausha-player.js&quot;&gt;&lt;/script&gt;

Lorena Pantano and Alexandra Bartlett dig into the tricks and tips that&apos;ll actually make your R code work better. We&apos;re talking about ditching those old habits we all picked up and switching to code that works better in 2025. We cover over 10 solid habits that&apos;ll seriously boost your R game - everything from how you&apos;re reading and storing files, making plots that are publish-ready, theming, data manipulation, and setting up environments so your code works when you come back to it later. If you want to up your R skills, this one&apos;s got practical stuff you can start using right away.

Listen to this podcast on other platforms:

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaApple size={32} className=&quot;text-black dark:text-white&quot; /&gt;{&apos; &apos;}
    &lt;a href=&quot;https://podcasts.apple.com/us/podcast/r-you-doing-it-right-modern-best-practices-in-r/id1817024741?i=1000719682944&quot;&gt;
      Apple
    &lt;/a&gt;
  &lt;/span&gt;

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaSpotify size={32} color=&quot;#1DB954&quot; /&gt;{&apos; &apos;}
    &lt;a href=&quot;https://open.spotify.com/episode/02XubBH33WzWzEfxUPR49G?si=7622cebba1e842ed&quot;&gt;Spotify&lt;/a&gt;
  &lt;/span&gt;

Send us your comments, questions, and suggestions using [this form](https://lnkd.in/eJ-hChm3)

We are looking for sponsors! Please get in touch if you or your business would like to help support this podcast.

Follow [Lorena](https://www.linkedin.com/in/lpantano) and [Alex](https://www.linkedin.com/in/alexandra-bartlett-926b32109) on LinkedIn!

If you enjoyed the episode, [please subscribe and leave a review!](https://lnkd.in/eXQ-HpUV)

Hosted by Ausha. See [ausha.co/privacy-policy](https://ausha.co/privacy-policy) for more information</description><pubDate>Tue, 29 Jul 2025 00:00:00 GMT</pubDate><category>coffee-with-compbio</category><category>computationalbiology</category><category>scicomm</category><category>podcast</category><category>bioinformatics</category><category>womeninscience</category><category>r-programming</category></item><item><title>Reactive vs Proactive Bioinformatics</title><link>https://boston-wib.org/blog/quicktake/reactive-vs-proactive-bioinformatics</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/reactive-vs-proactive-bioinformatics</guid><description>&lt;p&gt;&lt;em&gt;Bioinformatics enlightment occurs when one can regonize it has two main modes: reactive and proactive&lt;/em&gt;&lt;/p&gt;

Early in my career, I lived in &quot;reactive mode&quot;â€”scientists would come to me asking: &quot;Did this data accept or reject my hypothesis?&quot; or &quot;Where should I focus my next experiment?&quot;

It was exciting! I worked on diverse projects, collaborated with brilliant people, and contributed to breakthrough discoveries. Science moved fast, and I was right in the thick of it.

But I was also building a graveyard of one-off solutions. Beautiful analyses that answered important questions... once. Then got tossed aside for the next urgent request.

As I matured, I craved something different: proactive mode. Building rock-solid tools that would stand the test of time. Engineering solutions that got better with use, not abandoned after use.

Both approaches have their place, and I&apos;ve learned to recognize when each makes sense:

**REACTIVE MODE** works when:

- You&apos;re in discovery phase and don&apos;t know what questions you&apos;ll need to answer
- Science is moving faster than infrastructure can keep up
- You need to prove value before investing in scalable solutions

**PROACTIVE MODE** works when:

- You have recurring analytical needs that justify engineering investment
- The company is ready to think beyond the next experiment
- You want to empower scientists with self-service capabilities

The key insight? This isn&apos;t just about personal preferenceâ€”it&apos;s about organizational maturity.

Early-stage companies often NEED reactive bioinformaticians who can pivot quickly and answer urgent scientific questions. More mature companies benefit from proactive infrastructure that scales with their growing data needs.

The real skill is recognizing which mode your organization needs and adapting accordingly. Sometimes you need to be the firefighter, sometimes the architect.

Where do you find yourself today? Are you in reactive mode keeping up with urgent science, or proactive mode building for the future?</description><pubDate>Mon, 28 Jul 2025 00:00:00 GMT</pubDate><category>bioinformatics</category><category>career-development</category><category>data-science</category><category>biotech-careers</category><category>software-engineering</category></item><item><title>Design Docs for Bioinformatics</title><link>https://boston-wib.org/blog/quicktake/design-docs-for-bioinformatics</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/design-docs-for-bioinformatics</guid><description>&lt;p&gt;&lt;em&gt;Should we be designing bioinformatics projects like a software engineer?&lt;/em&gt;&lt;/p&gt;

I learned about &quot;Design Docs&quot; from software engineers, and it completely changed how I approach bioinformatics projects.

The concept: sit down, think it through, write it up, gather feedback, iterate... BEFORE you touch the keyboard.

My favorite section? &quot;Out of scope.&quot;

Because every bioinformatician knows this story: &quot;Can you do a quick analysis?&quot; turns into a three-month odyssey with no clear endpoint. The project balloons because nobody defined what we&apos;re NOT doing.

Here&apos;s why design docs could transform bioinformatics:

1. CRYSTALLIZE THE ACTUAL GOAL Instead of &quot;analyze the RNA-seq data,&quot; you write: &quot;Identify differentially expressed genes between treatment groups, focusing on immune pathways, to inform our next compound selection.&quot;

2. SURFACE DEPENDENCIES EARLY &quot;This analysis depends on completed sample QC and assumes we&apos;re using the latest genome build.&quot; No more surprises halfway through.

3. CREATE SHARED UNDERSTANDING When the wetlab scientist, the PI, and you all agree on the written scope, everyone&apos;s expectations are aligned.

Yes, it feels slower at first. &quot;Wait, you want feedback on my half-baked idea?!&quot;

But here&apos;s what actually happens: your half-baked idea becomes almost-fully-baked before you spend weeks implementing it. You catch scope creep before it catches you.

The design doc becomes your north star when stakeholders inevitably ask, &quot;While you&apos;re at it, could you also...&quot; You can point to the doc and say, &quot;That&apos;s out of scope for this analysis, but let&apos;s discuss it for the next one.&quot;

I wish they would teach us this in school. It would have saved me from so many &quot;quick analyses&quot; that turned into month-long rabbit holes.

Do you use any formal planning processes for your bioinformatics work? What keeps your analyses focused and scoped?</description><pubDate>Thu, 24 Jul 2025 08:00:00 GMT</pubDate><category>bioinformatics</category><category>project-management</category><category>data-science</category><category>software-engineering</category><category>research-workflow</category></item><item><title>Build vs Buy in Biotech</title><link>https://boston-wib.org/blog/quicktake/buildVsBuy</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/buildVsBuy</guid><description>&lt;p&gt;&lt;em&gt;The hidden costs of building vs buying in biotech&lt;/em&gt;&lt;/p&gt;

I&apos;ve lived on both sides of the &quot;build vs buy&quot; equation in biotech, and honestly? Both extremes taught me expensive lessons.

THE &quot;BUILD EVERYTHING&quot; COMPANY: We built our own LIMS, lab automation software, workflow orchestratorsâ€”everything powered by cloud infrastructure. It worked, but the maintenance burden was real.

THE &quot;BUY EVERYTHING&quot; COMPANY: Management thought we could just purchase our way to efficiency. The budget ballooned fast, and we still needed internal expertise to make anything work together.

Plot twist: None of our shiny new tools could talk to each other without expensive &quot;managed services.&quot; And guess what? We still needed internal project managers to coordinate with those managed services.

Here&apos;s what nobody puts in the budget: the hidden cost of integration ğŸ’¸ ğŸ’¸ ğŸ’¸

Buying a tool isn&apos;t buying a solutionâ€”it&apos;s buying a component that needs to fit into your ecosystem. That integration work? It still requires your people, your time, and your expertise.

The reality I&apos;ve learned: healthy companies live in the middle. Build your core differentiators, buy your commodity functions, but ALWAYS budget for the glue that holds it together.

The most successful biotech teams I&apos;ve seen ask different questions:

1. &quot;What gives us competitive advantage?&quot; (Build this)
2. &quot;What&apos;s table stakes for the industry?&quot; (Buy this, but budget for integration)
3. &quot;Do we have the expertise to maintain this long-term?&quot; (Be honest here)

The &quot;buy everything&quot; approach often comes from leadership who think technology problems can be solved with purchasing decisions. But integration, customization, and ongoing maintenance still require internal technical expertise.

You can&apos;t outsource your way out of needing to understand your own data infrastructure.

What&apos;s your experience with build vs buy in biotech? Where have you seen companies get this balance right (or wrong)?</description><pubDate>Mon, 21 Jul 2025 08:00:00 GMT</pubDate><category>biotech</category><category>tech-strategy</category><category>build-vs-buy</category><category>data-infrastructure</category><category>tech-leadership</category></item><item><title>Code Review Culture in Research Labs</title><link>https://boston-wib.org/blog/quicktake/code-review-culture-in-research-labs</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/code-review-culture-in-research-labs</guid><description>&lt;p&gt;&lt;em&gt;Not enough code review and you risk irreproducible science, too much and you kill discovery momentum.&lt;/em&gt;&lt;/p&gt;

Code review in research settings is a classic Goldilocks problem: too little and you risk irreproducible science, too much and you kill discovery momentum.

I&apos;ve experienced this challenge across teams of all sizes, and the solutions are surprisingly different:

- The Solo Bioinformatician Dilemma: You&apos;re embedded in a wetlab team. Who&apos;s your peer? The postdoc who knows R (while you code in Python)? The PI who coded in FORTRAN 20 years ago?

Honestly, I&apos;m still figuring this one out. Maybe external code review partnerships? Monthly virtual code clubs? I&apos;d love to hear how the community solves this.

- The stretched small team (3 people): Everyone&apos;s on different projects, everyone&apos;s oversubscribed. Code review feels like a luxury you can&apos;t afford.
  We tried it. It failed. The reality? When you&apos;re the only person who understands both the biology AND the pipeline, peer review becomes performative rather than protective.

- The sweet spot (5-6 people): This was where peer review actually workedâ€”but only because management explicitly protected our time for it. Key insight: leadership has to VALUE code review, not just require it.

We established &quot;review debt&quot; as a real metric. If your reviews were backlogged, you couldn&apos;t start new features. Sounds harsh, but it worked.

Here&apos;s what I learned: code review culture isn&apos;t just about catching bugs. It&apos;s about knowledge sharing, preventing single points of failure, and building team standards.

But in research, speed often beats perfection. The trick is finding review practices that ADD velocity instead of killing it.

Maybe we need research-specific review standards? Maybe pair programming works better than async reviews? Maybe some analyses deserve different review rigor than production pipelines?

What&apos;s your experience with code review in research settings? How do you balance rigor with discovery speed? And solo bioinformaticiansâ€”how do you handle this challenge?</description><pubDate>Fri, 18 Jul 2025 08:00:00 GMT</pubDate><category>bioinformatics</category><category>code-review</category><category>research-software</category><category>team-culture</category><category>software-development</category></item><item><title>The Bioinformatics Triangle: Memory, Elegance, and Speed</title><link>https://boston-wib.org/blog/quicktake/bioInfoTriangle</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/bioInfoTriangle</guid><description>&lt;p&gt;&lt;em&gt;How do you balance code aesthetics with performance in your bioinformatics workflows?&lt;/em&gt;&lt;/p&gt;

```python
# The Elegant Approach:
# Beautiful, concise, readable... but materializes all 64 codons in memory

pythonfrom itertools import product

codons = [&apos;&apos;.join(codon) for codon in product(&apos;ACGT&apos;, repeat=3)]

# The Memory-Efficient Approach:
# Constant memory usage, scales to millions of k-mers

pythonfor codon in product(&apos;ACGT&apos;, repeat=3):
    process(&apos;&apos;.join(codon)) # One at a time

# The Quick-and-Dirty Approach:
# Copy-paste ready, zero computation, maximum clarity

pythoncodons = [&apos;AAA&apos;, &apos;AAC&apos;, &apos;AAG&apos;, ...] # All codons hardcoded
```

Just had a fascinating discussion about generating all 64 possible codons in Python. Three approaches emerged:

1. The Elegant Approach: Beautiful, concise, readable... but materializes all 64 codons in memory
2. The Memory-Efficient Approach: Constant memory usage, scales to millions of k-mers
3. The Quick-and-Dirty Approach: Copy-paste ready, zero computation, maximum clarity

Here&apos;s the thing: in bioinformatics, we&apos;re constantly juggling massive datasets (think whole genomes), complex algorithms (phylogenetic trees, alignment scoring), and tight deadlines (grant applications, paper submissions).

For 64 codons? Any approach works fine. For analyzing all 15-mers in the human genome? That elegant list comprehension will crash your laptop. ğŸ’¥

The real skill isn&apos;t picking the &quot;right&quot; approachâ€”it&apos;s knowing when each approach fits. Sometimes you need the generator for scalability. Sometimes you need the hardcoded list for reliability. Sometimes you need the elegant one-liner for a quick analysis.

Where do you fall on this spectrum? Are you team &quot;premature optimization is evil&quot; or team &quot;memory efficiency from day one&quot;? How do you balance code aesthetics with performance in your bioinformatics workflows?</description><pubDate>Wed, 16 Jul 2025 00:00:00 GMT</pubDate><category>bioinformatics</category><category>python</category><category>computational-biology</category><category>coding</category><category>data-science</category></item><item><title>The Power of Strategic Data Infrastructure</title><link>https://boston-wib.org/blog/quicktake/dataInfrastructure</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/dataInfrastructure</guid><description>&lt;p&gt;&lt;em&gt;Great science happens when technical infrastructure meets scientific curiosity&lt;/em&gt;&lt;/p&gt;

Sometimes the best features come from a single sentence in a meeting.

I was sitting with our wetlab director when she mentioned, almost in passing: &quot;I wish we could see how our controls have performed historically...&quot;

My brain lit up. &quot;Waitâ€”you want this data? I HAVE this data!&quot;

Because we&apos;d built our data warehouse to be comprehensive from day one, this &quot;wish&quot; became reality in under a week. One new tab in our visualization app, and suddenly scientists could track control performance trends across months of experiments.

Here&apos;s what made this magic possible:

- Strategic infrastructure pays dividends: We didn&apos;t just build for today&apos;s requirementsâ€”we captured everything we could think of, knowing future questions would emerge. That upfront investment in comprehensive data modeling meant we could pivot to new insights instantly.

- Low effort, high impact: The technical lift was minimal because the foundation was solid. No emergency data migrations, no rushed ETL pipelines. Just a new query and some charts.

- Collaboration creates breakthroughs: Here&apos;s my favorite part: I never thought to surface this data on my own. It took a scientist&apos;s domain expertise to recognize the value hidden in our warehouse.

This is how great science happensâ€”when technical infrastructure meets scientific curiosity. The data was always there, but it took cross-functional conversation to unlock its potential.

The lesson? Build your data foundation wide and deep. You never know which &quot;I wish we could see...&quot; will become your next game-changing feature.

And always, ALWAYS listen for those casual comments in meetings. They&apos;re often treasure maps to your next big win.

What unexpected insights have emerged from your data infrastructure? What &quot;casual wishes&quot; turned into powerful features?</description><pubDate>Tue, 15 Jul 2025 00:00:00 GMT</pubDate><category>biotech</category><category>data-warehouse</category><category>cross-functional</category><category>data-strategy</category><category>scientific-computing</category></item><item><title>Why Computational Biologists Need Lab Notebooks</title><link>https://boston-wib.org/blog/quicktake/computationalLabNotebooks</link><guid isPermaLink="true">https://boston-wib.org/blog/quicktake/computationalLabNotebooks</guid><description>&lt;p&gt;&lt;em&gt;Somehow, this fundamental practice gets lost when we move to computational biology&lt;/em&gt;&lt;/p&gt;

Biologists learn to keep lab notebooks in Bio 101. They document experimental designs, observations, what worked, what didn&apos;t.

But somehow, this fundamental practice gets lost when we move to computational biology.

I&apos;ve met countless bioinformaticians and data scientists who can tell you the exact pH of their last buffer, but can&apos;t remember why they chose specific parameters for an analysis they ran last month.

Here&apos;s why I think every computational biologist should keep a lab notebook:

â¡ï¸ SCENARIO: You run a complex command line tool with 15 parameters. Six months later, you need to reproduce the analysis on a similar dataset.

â¡ï¸ WITHOUT A NOTEBOOK: You&apos;re digging through 10,000 lines of bash history, trying to remember if you used --min-coverage 10 or 20, and WHY you made that choice.

â¡ï¸ WITH A NOTEBOOK: &quot;Tried --min-coverage 10 initially but got too much noise in low-quality regions. Switched to 20 based on Smith et al. 2023 recommendation for similar tissue type.&quot;

The magic isn&apos;t just recording WHAT you didâ€”it&apos;s capturing WHY you did it.
When you document your rationale in real-time, you&apos;re not just helping future you. You&apos;re building institutional knowledge that can be shared, reviewed, and improved upon.

Your notebook becomes a roadmap for scaling analyses, training team members, and catching edge cases before they become problems.

We wouldn&apos;t accept a wetlab scientist who couldn&apos;t reproduce their experiments. Why do we accept computational work that can&apos;t be reproduced?

The best part? Your &quot;lab notebook&quot; can be as simple as a markdown file alongside your code. No fancy tools required. (although I personally am a Confluence fan girl ğŸ¤“ )

Do you keep a computational lab notebook? What&apos;s your system for documenting analysis decisions?</description><pubDate>Fri, 11 Jul 2025 00:00:00 GMT</pubDate><category>bioinformatics</category><category>data-science</category><category>computational-biology</category><category>reproducibility</category><category>best-practices</category></item><item><title>A Coffee with CompBio: Fast, Private, and Publish-Ready Spatial Transcriptomics App (Without Losing Your Mind)</title><link>https://boston-wib.org/blog/coffeewithcompbio/episode-4</link><guid isPermaLink="true">https://boston-wib.org/blog/coffeewithcompbio/episode-4</guid><description>&lt;p&gt;&lt;em&gt;Alex and Lorena journey through the real-life challenges of building interactive single cell spatial data visualizations for large projects.&lt;/em&gt;&lt;/p&gt;

import { FaBell, FaSpotify, FaApple } from &apos;react-icons/fa&apos;;

&lt;iframe
  name=&quot;Ausha Podcast Player&quot;
  frameborder=&quot;0&quot;
  loading=&quot;lazy&quot;
  id=&quot;ausha-lG8A&quot;
  height=&quot;220&quot;
  style=&quot;border: none; width:100%; height:220px&quot;
  src=&quot;https://player.ausha.co/?podcastId=reGGWuRj47pL&amp;playlist=false&amp;color=%23751cbf&amp;v=3&amp;playerId=ausha-lG8A&quot;
&gt;&lt;/iframe&gt;
&lt;script src=&quot;https://player.ausha.co/ausha-player.js&quot;&gt;&lt;/script&gt;

In this episode, we journey through the real-life challenges of building interactive single cell spatial data visualizations for large projects. Lorena shares her recent adventure turning mountains of data into a web app using tools like Python, R, and the (tricky-to-pronounce) single-cell viewer [_Vitessce_](https://vitessce.io/). She discusses the hurdles of image cropping, memory limits, Python-R crossovers, and why â€œjust putting it onlineâ€ isnâ€™t as easy as it soundsâ€”especially when it comes to privacy, deployment, and avoiding surprise cloud bills. If youâ€™ve ever had a collaborator say, &quot;Can you just build me an app I can play with?&quot;, this episode is for you.

In the &quot;**Quick Sip**&quot; segment, Alex and Lorena share tips on automating code linting with GitHub Actions. Finally, in our &quot;**Brewing Up Answers**&quot; segment, we chat about managing people in academia vs. industry, and why itâ€™s a very different ballgame on each side of the fence.

Listen to this podcast on other platforms:

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaApple size={32} className=&quot;text-black dark:text-white&quot; /&gt;{&apos; &apos;}
    &lt;a href=&quot;https://podcasts.apple.com/us/podcast/fast-private-and-publish-ready-spatial/id1817024741?i=1000716612827&quot;&gt;
      Apple
    &lt;/a&gt;
  &lt;/span&gt;

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaSpotify size={32} color=&quot;#1DB954&quot; /&gt;{&apos; &apos;}
    &lt;a href=&quot;https://open.spotify.com/episode/3b0KdhZJr4mhnPGixu1Fkm?si=2HeKdAU5T5qWbPgO0Fur4A&quot;&gt;Spotify&lt;/a&gt;
  &lt;/span&gt;

Send us your comments, questions, and suggestions using [this form](https://lnkd.in/eJ-hChm3)

We are looking for sponsors! Please get in touch if you or your business would like to help support this podcast.

Follow [Lorena](https://www.linkedin.com/in/lpantano) and [Alex](https://www.linkedin.com/in/alexandra-bartlett-926b32109) on LinkedIn!

If you enjoyed the episode, [please subscribe and leave a review!](https://lnkd.in/eXQ-HpUV)

Hosted by Ausha. See [ausha.co/privacy-policy](https://ausha.co/privacy-policy) for more information</description><pubDate>Thu, 10 Jul 2025 00:00:00 GMT</pubDate><category>coffee-with-compbio</category><category>computational-biology</category><category>podcast</category><category>bioinformatics</category><category>womeninscience</category><category>quick-sips</category><category>brewing-up-for-answers</category><category>github-actions</category><category>github</category><category>academia-industry</category><category>cloud-computing</category><category>web-app</category><category>vitessce</category><category>python</category><category>r-programming</category><category>spatial-transcriptomics</category></item><item><title>A Coffee with CompBio: R Markdown</title><link>https://boston-wib.org/blog/coffeewithcompbio/episode-3</link><guid isPermaLink="true">https://boston-wib.org/blog/coffeewithcompbio/episode-3</guid><description>&lt;p&gt;&lt;em&gt;Alex and Lorena discuss a large bulk RNA-seq project that yielded lasting changes to their groupâ€™s everyday bioinformatics practices via the creation of parameterized R Markdown code templates.&lt;/em&gt;&lt;/p&gt;

import { FaBell, FaSpotify, FaApple } from &apos;react-icons/fa&apos;;

&lt;iframe
  name=&quot;Ausha Podcast Player&quot;
  frameborder=&quot;0&quot;
  loading=&quot;lazy&quot;
  id=&quot;ausha-qiIA&quot;
  height=&quot;220&quot;
  style=&quot;border: none; width:100%; height:220px&quot;
  src=&quot;https://player.ausha.co/?podcastId=9G22jH5n0lXZ&amp;playlist=false&amp;color=%23751cbf&amp;v=3&amp;playerId=ausha-qiIA&quot;
&gt;&lt;/iframe&gt;
&lt;script src=&quot;https://player.ausha.co/ausha-player.js&quot;&gt;&lt;/script&gt;

Lorena Pantano and Alexandra Bartlett discuss a large bulk RNA-seq project that yielded lasting changes to their groupâ€™s everyday bioinformatics practices via the creation of parameterized R Markdown code templates. In the &quot;**Quick Sip**&quot; segment, they discuss [_reticulate_](https://rstudio.github.io/reticulate/) for managing python environments in an R context, and in &quot;**Brewing Up Answers**&quot;, they reflect on the differences between industry and academia bioinformatics.

Listen to this podcast on other platforms:

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaApple size={32} classname=&quot;text-black dark:text-white&quot; /&gt;{&apos; &apos;}
    &lt;a href=&quot;https://podcasts.apple.com/us/podcast/r-markdown-because-rna-seq-code-shouldnt-be-wild-type/id1817024741?i=1000714675040&quot;&gt;
      Apple
    &lt;/a&gt;
  &lt;/span&gt;

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaSpotify size={32} color=&quot;#1DB954&quot; /&gt;{&apos; &apos;}
    &lt;a href=&quot;https://open.spotify.com/episode/3y3Xp1XUsVbrbNSGiQLAPo?si=ec0cbd69073246ab&quot;&gt;Spotify&lt;/a&gt;
  &lt;/span&gt;

Send us your comments, questions, and suggestions using [this form](https://lnkd.in/eJ-hChm3)

We are looking for sponsors! Please get in touch if you or your business would like to help support this podcast.

Follow [Lorena](https://www.linkedin.com/in/lpantano) and [Alex](https://www.linkedin.com/in/alexandra-bartlett-926b32109) on LinkedIn!

If you enjoyed the episode, [please subscribe and leave a review!](https://lnkd.in/eXQ-HpUV)

Hosted by Ausha. See [ausha.co/privacy-policy](https://ausha.co/privacy-policy) for more information</description><pubDate>Thu, 26 Jun 2025 00:00:00 GMT</pubDate><category>coffee-with-compbio</category><category>computational-biology</category><category>podcast</category><category>bioinformatics</category><category>womeninscience</category><category>quick-sips</category><category>brewing-up-for-answers</category><category>reticulate</category><category>r-markdown</category><category>rna-sequencing</category><category>python</category><category>r-programming</category></item><item><title>A Coffee with CompBio: The Thousand Dollar Alignment</title><link>https://boston-wib.org/blog/coffeewithcompbio/episode-2</link><guid isPermaLink="true">https://boston-wib.org/blog/coffeewithcompbio/episode-2</guid><description>&lt;p&gt;&lt;em&gt;From puzzlingly low mapping rates to unexpected cloud costs caused by unoptimized compute jobs, Lorena and Alex highlight how essential clear communication and bioinformatics-aware experimental design are to any successful project.&lt;/em&gt;&lt;/p&gt;

import { FaBell, FaSpotify, FaApple } from &apos;react-icons/fa&apos;;

&lt;iframe
  name=&quot;Ausha Podcast Player&quot;
  frameborder=&quot;0&quot;
  loading=&quot;lazy&quot;
  id=&quot;ausha-RqFX&quot;
  height=&quot;220&quot;
  style=&quot;border: none; width:100%; height:220px&quot;
  src=&quot;https://player.ausha.co/?podcastId=xAGG0uwpe3d8&amp;playlist=false&amp;color=%23751cbf&amp;v=3&amp;playerId=ausha-RqFX&quot;
&gt;&lt;/iframe&gt;
&lt;script src=&quot;https://player.ausha.co/ausha-player.js&quot;&gt;&lt;/script&gt;

Lorena Pantano and Alexandra Bartlett share the twists and turns of realizing their methylation data wasnâ€™t what it seemed. From puzzlingly low mapping rates to unexpected cloud costs caused by unoptimized compute jobsâ€”thankfully caught just in time thanks to cost alarmsâ€”they highlight how essential clear communication and bioinformatics-aware experimental design are to any successful project.

In our new segments, **Quick Sips** and **Brewing Up for Answers**, we talk about [PIXI](https://pixi.sh/latest/) for managing software environments (Thanks to Edmund Miller) and dig into the ever-present challenge of staying organized across complex projects.

Listen to this podcast on other platforms:

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaApple size={32} classname=&quot;text-black dark:text-white&quot; /&gt; &lt;a href=&quot;https://apple.co/4dXieNl&quot;&gt;Apple&lt;/a&gt;
  &lt;/span&gt;

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaSpotify size={32} color=&quot;#1DB954&quot; /&gt; &lt;a href=&quot;https://spoti.fi/45Np6ur&quot;&gt;Spotify&lt;/a&gt;
  &lt;/span&gt;

Send us your comments, questions, and suggestions using [this form](https://lnkd.in/eJ-hChm3)

We are looking for sponsors! Please get in touch if you or your business would like to help support this podcast.

Follow [Lorena](https://www.linkedin.com/in/lpantano) and [Alex](https://www.linkedin.com/in/alexandra-bartlett-926b32109) on LinkedIn!

If you enjoyed the episode, [please subscribe and leave a review!](https://lnkd.in/eXQ-HpUV)

Hosted by Ausha. See [ausha.co/privacy-policy](https://ausha.co/privacy-policy) for more information</description><pubDate>Tue, 10 Jun 2025 00:00:00 GMT</pubDate><category>coffee-with-compbio</category><category>computationalbiology</category><category>podcast</category><category>science-communication</category><category>bioinformatics</category><category>womeninscience</category><category>pixi</category><category>reproducible-research</category><category>cloud-computing</category><category>quick-sips</category><category>brewing-up-for-answers</category></item><item><title>A Coffee with CompBio: Nine Samples and Zero Cells</title><link>https://boston-wib.org/blog/coffeewithcompbio/episode-1</link><guid isPermaLink="true">https://boston-wib.org/blog/coffeewithcompbio/episode-1</guid><description>&lt;p&gt;&lt;em&gt;Alex and Lorena dive into the messy reality of processing single-cell RNA-seq data.&lt;/em&gt;&lt;/p&gt;

import { FaBell, FaSpotify, FaApple } from &apos;react-icons/fa&apos;;

&lt;iframe
  name=&quot;Ausha Podcast Player&quot;
  frameborder=&quot;0&quot;
  loading=&quot;lazy&quot;
  id=&quot;ausha-AXeu&quot;
  height=&quot;220&quot;
  style=&quot;border: none; width:100%; height:220px&quot;
  src=&quot;https://player.ausha.co/?podcastId=9G22jHrpQ7n4&amp;playlist=false&amp;color=%23751cbf&amp;v=3&amp;playerId=ausha-AXeu&quot;
&gt;&lt;/iframe&gt;
&lt;script src=&quot;https://player.ausha.co/ausha-player.js&quot;&gt;&lt;/script&gt;

In our first episode, Alex and Lorena dive into the messy reality of processing single-cell RNA-seq data. What started as a simple QC project turned into a week-long journey across compute environments, mysterious pipeline errors, and zero-cell outputs. Along the way, we troubleshoot issues with Cell Ranger, uncover strange sequencing artifacts, and reflect on lessons in data handling, pipeline reproducibility, and client communication.

Listen to this podcast on other platforms:

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaApple size={32} classname=&quot;text-black dark:text-white&quot; /&gt;{&apos; &apos;}
    &lt;a href=&quot;https://podcasts.apple.com/us/podcast/nine-samples-and-zero-cells-a-week-in-the-life/id1817024741?i=1000710174476&quot;&gt;
      Apple
    &lt;/a&gt;
  &lt;/span&gt;

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaSpotify size={32} color=&quot;#1DB954&quot; /&gt;{&apos; &apos;}
    &lt;a href=&quot;https://open.spotify.com/episode/4XF2yQ70NXlgFHxlNOKbnb?si=aa9737c9f1c2411e&quot;&gt;Spotify&lt;/a&gt;
  &lt;/span&gt;

Send us your comments, questions, and suggestions using [this form](https://lnkd.in/eJ-hChm3)

We are looking for sponsors! Please get in touch if you or your business would like to help support this podcast.

Follow [Lorena](https://www.linkedin.com/in/lpantano) and [Alex](https://www.linkedin.com/in/alexandra-bartlett-926b32109) on LinkedIn!

If you enjoyed the episode, [please subscribe and leave a review!](https://lnkd.in/eXQ-HpUV)

Hosted by Ausha. See [ausha.co/privacy-policy](https://ausha.co/privacy-policy) for more information</description><pubDate>Tue, 27 May 2025 00:00:01 GMT</pubDate><category>coffee-with-compbio</category><category>computational-biology</category><category>podcast</category><category>science-communication</category><category>bioinformatics</category><category>womeninscience</category><category>single-cell</category><category>rna-sequencing</category><category>troubleshooting</category><category>cell-ranger</category></item><item><title>A Coffee with CompBio: Intro</title><link>https://boston-wib.org/blog/coffeewithcompbio/episode-0</link><guid isPermaLink="true">https://boston-wib.org/blog/coffeewithcompbio/episode-0</guid><description>&lt;p&gt;&lt;em&gt;Whether you&apos;re a researcher, student, or just bio-curious, join us for casual, insightful conversations that bridge science and real life.&lt;/em&gt;&lt;/p&gt;

import { FaBell, FaSpotify, FaApple } from &apos;react-icons/fa&apos;;

&lt;iframe
  name=&quot;Ausha Podcast Player&quot;
  frameborder=&quot;0&quot;
  loading=&quot;lazy&quot;
  id=&quot;ausha-kP84&quot;
  height=&quot;220&quot;
  style=&quot;border: none; width:100%; height:220px&quot;
  src=&quot;https://player.ausha.co/?podcastId=qxaaQs4Xjwkv&amp;v=3&amp;playerId=ausha-kP84&quot;
&gt;&lt;/iframe&gt;
&lt;script src=&quot;https://player.ausha.co/ausha-player.js&quot;&gt;&lt;/script&gt;

In the introductory episode, Lorena and Alex introduce themselves and share how they got started in computational biology. They talk about their career paths, what drew them to bioinformatics, and some of the challenges and surprises theyâ€™ve encountered along the way. They also give a preview of the kinds of topics and practical issues theyâ€™ll be covering on the podcast, from workflow basics to troubleshooting analysis hiccups.

Listen to this podcast on other platforms:

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaApple size={32} classname=&quot;text-black dark:text-white&quot; /&gt;{&apos; &apos;}
    &lt;a href=&quot;https://podcasts.apple.com/us/podcast/about-us/id1817024741?i=1000710167550&quot;&gt;Apple&lt;/a&gt;
  &lt;/span&gt;

- &lt;span style={{ display: &apos;inline-flex&apos;, alignItems: &apos;center&apos;, gap: &apos;0.5em&apos; }}&gt;
    &lt;FaSpotify size={32} color=&quot;#1DB954&quot; /&gt;{&apos; &apos;}
    &lt;a href=&quot;https://open.spotify.com/episode/2zrTh0TGfuoW5flj2bO0hR?si=cXBcLwj3SB2h7ADbQ7MYSQ&quot;&gt;Spotify&lt;/a&gt;
  &lt;/span&gt;

Hosted by Ausha. See [ausha.co/privacy-policy](https://ausha.co/privacy-policy) for more information</description><pubDate>Tue, 27 May 2025 00:00:00 GMT</pubDate><category>coffee-with-compbio</category><category>computationalbiology</category><category>scicomm</category><category>podcast</category><category>sciencecommunication</category><category>bioinformatics</category><category>womeninscience</category></item></channel></rss>